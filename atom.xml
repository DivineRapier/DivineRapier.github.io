<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>お可愛いこと</title>
  
  
  <link href="http://divinerapier.github.io/atom.xml" rel="self"/>
  
  <link href="http://divinerapier.github.io/"/>
  <updated>2020-11-14T14:13:33.889Z</updated>
  <id>http://divinerapier.github.io/</id>
  
  <author>
    <name>A</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Nvidia Docker</title>
    <link href="http://divinerapier.github.io/2020/11/14/nvidia-docker/"/>
    <id>http://divinerapier.github.io/2020/11/14/nvidia-docker/</id>
    <published>2020-11-14T10:21:01.000Z</published>
    <updated>2020-11-14T14:13:33.889Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/using-nvidia-gpu-on-kubernetes/nvidia-container-toolkit.png" alt="nvidia-container-toolkit.png"></p><p>Nvidia Container Toolkit 包含容器运行时库和一些工具，用于自动配置容器使用 GPU 资源。并且，支持多种不同的容器引擎，如 Docker、LXC、Podman 等。用户根据需要可以自行选择使用哪种引擎。</p><h2 id="The-Architecture-Overview-of-Nvidia-Container-Toolkit"><a href="#The-Architecture-Overview-of-Nvidia-Container-Toolkit" class="headerlink" title="The Architecture Overview of Nvidia Container Toolkit"></a>The Architecture Overview of Nvidia Container Toolkit</h2><p>Nvidia Container Toolkit 的架构允许其支持任何容器运行时。若以 Docker 为例，其由以下组件，以从上到下的层次结构组成:</p><ul><li>nvidia-docker2</li><li>nvidia-container-runtime</li><li>nvidia-container-toolkit</li><li>libnvidia-container</li></ul><p>下图为各个组件的关系:</p><p><img src="/images/using-nvidia-gpu-on-kubernetes/nvidia-docker-arch.png" alt="nvidia-docker-arch.png"></p><h3 id="Components-and-Packages"><a href="#Components-and-Packages" class="headerlink" title="Components and Packages"></a>Components and Packages</h3><h4 id="libnvidia-container"><a href="#libnvidia-container" class="headerlink" title="libnvidia-container"></a>libnvidia-container</h4><p>提供库与 CLI 程序，实现自动化配置 GNU/Linux 容器使用 NVIDIA GPU 资源，其实现依赖于内核基础功能，且在设计上与容器运行时解耦。</p><p>libnvidia-container 提供了一个定义良好的 API 和一个封装好的 CLI 程序(nvidia-container-cli)，任何容器运行时都可以调用它来支持 NVIDIA GPU。</p><h4 id="nvidia-container-toolkit"><a href="#nvidia-container-toolkit" class="headerlink" title="nvidia-container-toolkit"></a>nvidia-container-toolkit</h4><p>实现了 runC prestart hook 需要的接口的脚本。该脚本在容器被创建之后，启动之前被 runC 调用，且被赋予访问与容器相关联的 config.json 的权限。脚本根据 config.json 中的信息作为合适的命令行参数 (an appropriate set of flags) 来调用 libnvidia-container CLI。其中，“指定哪些 GPU 设备在容器中使用” 是最重要的参数。</p><p>该组件之前的名字是 nvidia-container-runtime-hook，现在系统上的 nvidia-container-runtime-hook 是 nvidia-container-toolkit 的符号链接。</p><h4 id="nvidia-container-runtime"><a href="#nvidia-container-runtime" class="headerlink" title="nvidia-container-runtime"></a>nvidia-container-runtime</h4><p>曾经，nvidia-container-runtime 以 runC 作为基础，添加了 NVIDIA 特定的代码。2019 年，更改为对宿主机上原生 runC 做简单的封装。nvidia-container-runtime 将 runC spec 作为输入，将 nvidia-container-toolkit 脚本作为 prestart hook 注入到 runC spec 中。然后，将修改后的带有该 hook set 的 runC spec 传递给原生 runC 并调用 runC。需要注意的是，该组件不一定是针对 docker 的(但它是针对runC的)。</p><p>当该 package 完成安装后，Docker 的 daemon.json 文件会被更新为指向这个二进制文件:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ cat /etc/docker/daemon.json</span><br><span class="line">&#123;</span><br><span class="line"><span class="string">"default-runtime"</span>: <span class="string">"nvidia"</span>,</span><br><span class="line"><span class="string">"runtimes"</span>: &#123;</span><br><span class="line">    <span class="string">"nvidia"</span>: &#123;</span><br><span class="line">        <span class="string">"path"</span>: <span class="string">"/usr/bin/nvidia-container-runtime"</span>,</span><br><span class="line">        <span class="string">"runtimeArgs"</span>: []</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="nvidia-docker2"><a href="#nvidia-docker2" class="headerlink" title="nvidia-docker2"></a>nvidia-docker2</h4><p>这个 package 是架构中唯一的 docker 专用包。它采用与 nvidia-container-runtime 相关的脚本，并将其安装到 docker 的 /etc/docker/daemon.json 文件中。这样，使用者就可以运行 <strong>docker run –runtime=nvidia …</strong> 来自动为容器添加对 GPU 的支持。这个 package 还安装了一个封装了原生 docker CLI 的脚本，名为 nvidia-docker，避免每次都指定 –runtime=nvidia 来调用 docker。它还允许用户在宿主机上设置环境变量 NV_GPU 来指定将哪些 GPU 注入到容器中。</p><h2 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h2><h3 id="Pre-Requisites"><a href="#Pre-Requisites" class="headerlink" title="Pre-Requisites"></a>Pre-Requisites</h3><ul><li><a href="https://www.nvidia.com/Download/index.aspx?lang=en-us" target="_blank" rel="noopener">NVIDIA Drivers</a></li><li>Platform Requirements:<ol><li>GNU/Linux x86_64 with kernel version &gt; 3.10</li><li>Docker &gt;= 19.03 (recommended, but some distributions may include older versions of Docker. The minimum supported version is 1.12)</li><li>NVIDIA GPU with Architecture &gt; Fermi (or compute capability 2.1)</li><li>NVIDIA drivers ~= 361.93 (untested on older versions)</li></ol></li><li>Docker CE</li></ul><h3 id="Setting-up-NVIDIA-Container-Toolkit"><a href="#Setting-up-NVIDIA-Container-Toolkit" class="headerlink" title="Setting up NVIDIA Container Toolkit"></a>Setting up NVIDIA Container Toolkit</h3><p>安装软件源与 GPG key:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ distribution=$(. /etc/os-release;<span class="built_in">echo</span> <span class="variable">$ID</span><span class="variable">$VERSION_ID</span>) \</span><br><span class="line">   &amp;&amp; curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - \</span><br><span class="line">   &amp;&amp; curl -s -L https://nvidia.github.io/nvidia-docker/<span class="variable">$distribution</span>/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list</span><br></pre></td></tr></table></figure><p>安装 nvidia-docker2 并重启 Docker Daemon:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get update \</span><br><span class="line">   &amp;&amp; sudo apt-get install -y nvidia-docker2 \</span><br><span class="line">   &amp;&amp; sudo systemctl restart docker</span><br></pre></td></tr></table></figure><p>启动容器测试，如果得到类似如下的输出则安装成功:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi</span><br><span class="line"></span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI 450.51.06    Driver Version: 450.51.06    CUDA Version: 11.0     |</span><br><span class="line">|-------------------------------+----------------------+----------------------+</span><br><span class="line">| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|                               |                      |               MIG M. |</span><br><span class="line">|===============================+======================+======================|</span><br><span class="line">|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |</span><br><span class="line">| N/A   34C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |</span><br><span class="line">|                               |                      |                  N/A |</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line"></span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| Processes:                                                                  |</span><br><span class="line">|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |</span><br><span class="line">|        ID   ID                                                   Usage      |</span><br><span class="line">|=============================================================================|</span><br><span class="line">|  No running processes found                                                 |</span><br><span class="line">+-----------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="https://docs.nvidia.com/datacenter/cloud-native/index.html" target="_blank" rel="noopener">NVIDIA Cloud Native Technologies</a></li><li><a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html" target="_blank" rel="noopener">Container Toolkit Installation Guide</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/images/using-nvidia-gpu-on-kubernetes/nvidia-container-toolkit.png&quot; alt=&quot;nvidia-container-toolkit.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;Nvidia Containe</summary>
      
    
    
    
    
    <category term="gpu" scheme="http://divinerapier.github.io/tags/gpu/"/>
    
    <category term="docker" scheme="http://divinerapier.github.io/tags/docker/"/>
    
    <category term="nvidia" scheme="http://divinerapier.github.io/tags/nvidia/"/>
    
  </entry>
  
  <entry>
    <title>为 Linux 增加新磁盘</title>
    <link href="http://divinerapier.github.io/2020/11/01/adding-a-new-hard-drive-for-linux/"/>
    <id>http://divinerapier.github.io/2020/11/01/adding-a-new-hard-drive-for-linux/</id>
    <published>2020-11-01T12:41:21.000Z</published>
    <updated>2020-11-01T14:22:45.887Z</updated>
    
    <content type="html"><![CDATA[<h2 id="查看设备文件"><a href="#查看设备文件" class="headerlink" title="查看设备文件"></a>查看设备文件</h2><p>将磁盘插入计算机后，在终端中查看:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ lsblk -d -o name,serial</span><br><span class="line"></span><br><span class="line">NAME    SERIAL</span><br><span class="line">nvme0n1 200000000000</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ sudo fdisk -l</span><br><span class="line"></span><br><span class="line">Disk /dev/nvme0n1: 931.53 GiB, 1000204886016 bytes, 1953525168 sectors</span><br><span class="line">Disk model: WDS100T3X0C-00SJG0</span><br><span class="line">Units: sectors of 1 * 512 = 512 bytes</span><br><span class="line">Sector size (logical/physical): 512 bytes / 512 bytes</span><br><span class="line">I/O size (minimum/optimal): 512 bytes / 512 bytes</span><br></pre></td></tr></table></figure><p>可以确认设备文件为 <code>/dev/nvme0n1</code>。</p><h2 id="创建分区"><a href="#创建分区" class="headerlink" title="创建分区"></a>创建分区</h2><p>创建分区表:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo parted /dev/nvme0n1 mklabel gpt</span><br><span class="line"></span><br><span class="line">Information: You may need to update /etc/fstab.</span><br></pre></td></tr></table></figure><p>创建主分区，并确认对齐:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo parted -s -m /dev/nvme0n1 mkpart primary ext4 1 100%</span><br><span class="line"></span><br><span class="line">$ sudo parted /dev/nvme0n1 align-check opt 1</span><br><span class="line">1 aligned</span><br></pre></td></tr></table></figure><p>查看分区详细信息:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ sudo parted /dev/nvme0n1 <span class="built_in">print</span></span><br><span class="line">Model: WDS100T3X0C-00SJG0 (nvme)</span><br><span class="line">Disk /dev/nvme0n1: 1000GB</span><br><span class="line">Sector size (logical/physical): 512B/512B</span><br><span class="line">Partition Table: gpt</span><br><span class="line">Disk Flags:</span><br><span class="line"></span><br><span class="line">Number  Start   End     Size    File system  Name     Flags</span><br><span class="line"> 1      1049kB  1000GB  1000GB               primary</span><br></pre></td></tr></table></figure><h2 id="使用磁盘"><a href="#使用磁盘" class="headerlink" title="使用磁盘"></a>使用磁盘</h2><p>创建文件系统:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ sudo mkfs.ext4 /dev/nvme0n1p1</span><br><span class="line"></span><br><span class="line">mke2fs 1.45.5 (07-Jan-2020)</span><br><span class="line">Discarding device blocks: <span class="keyword">done</span></span><br><span class="line">Creating filesystem with 244190208 4k blocks and 61054976 inodes</span><br><span class="line">Filesystem UUID: b5424944-2d8c-4c5f-8bb4-0e538db5592b</span><br><span class="line">Superblock backups stored on blocks:</span><br><span class="line">        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208,</span><br><span class="line">        4096000, 7962624, 11239424, 20480000, 23887872, 71663616, 78675968,</span><br><span class="line">        102400000, 214990848</span><br><span class="line"></span><br><span class="line">Allocating group tables: <span class="keyword">done</span></span><br><span class="line">Writing inode tables: <span class="keyword">done</span></span><br><span class="line">Creating journal (262144 blocks): <span class="keyword">done</span></span><br><span class="line">Writing superblocks and filesystem accounting information: <span class="keyword">done</span></span><br></pre></td></tr></table></figure><p>挂载磁盘:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ sudo mkdir -p /nvme</span><br><span class="line"></span><br><span class="line">$ sudo mount /dev/nvme0n1p1 /nvme</span><br><span class="line"></span><br><span class="line">$ df -h</span><br><span class="line">Filesystem      Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/nvme0n1p1  916G   77M  870G   1% /nvme</span><br></pre></td></tr></table></figure><h2 id="测试磁盘"><a href="#测试磁盘" class="headerlink" title="测试磁盘"></a>测试磁盘</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /nvme</span><br><span class="line"></span><br><span class="line"><span class="comment"># ioengine: 可以指定为 psync / libaio</span></span><br><span class="line"><span class="comment"># numjobs: 测试线程数，线程之间的测试相互独立，成倍占用 size 指定的大小</span></span><br><span class="line"><span class="comment"># rw: 读写方式</span></span><br><span class="line"><span class="comment">#     read: 顺序读</span></span><br><span class="line"><span class="comment">#     write: 顺序写</span></span><br><span class="line"><span class="comment">#     randread: 随机读</span></span><br><span class="line"><span class="comment">#     randwrite: 随机写</span></span><br><span class="line"><span class="comment"># bs: 每次读写块大小</span></span><br><span class="line">$ sudo fio -direct=1 -iodepth 1 -thread -rw=randread -ioengine=psync -bs=4k -size=100G -numjobs=4 -group_reporting -name=file</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;查看设备文件&quot;&gt;&lt;a href=&quot;#查看设备文件&quot; class=&quot;headerlink&quot; title=&quot;查看设备文件&quot;&gt;&lt;/a&gt;查看设备文件&lt;/h2&gt;&lt;p&gt;将磁盘插入计算机后，在终端中查看:&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;</summary>
      
    
    
    
    
    <category term="linux" scheme="http://divinerapier.github.io/tags/linux/"/>
    
    <category term="hard drive" scheme="http://divinerapier.github.io/tags/hard-drive/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes DaemonSet</title>
    <link href="http://divinerapier.github.io/2020/10/28/kubernetes-daemonset/"/>
    <id>http://divinerapier.github.io/2020/10/28/kubernetes-daemonset/</id>
    <published>2020-10-28T06:41:29.000Z</published>
    <updated>2020-10-28T08:13:15.385Z</updated>
    
    <content type="html"><![CDATA[<p><strong>DaemonSet</strong> 确保全部 (或者某些) 节点上运行一个 Pod 的副本。 当有节点加入集群时，也会为他们新增一个 Pod 。当有节点从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除它创建的所有 Pod。</p><p>DaemonSet 的一些典型用法:</p><ul><li>在每个节点上运行集群守护进程<ul><li>比如: 网络插件，存储插件</li></ul></li><li>在每个节点上运行日志收集守护进程</li><li>在每个节点上运行监控守护进程</li></ul><h2 id="创建-DaemonSet"><a href="#创建-DaemonSet" class="headerlink" title="创建 DaemonSet"></a>创建 DaemonSet</h2><p>下面的 <strong>daemonset.yaml</strong> 文件描述了一个运行 <strong>fluentd-elasticsearch</strong> Docker 镜像的 DaemonSet:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">DaemonSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">fluentd-elasticsearch</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">fluentd-logging</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">fluentd-elasticsearch</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">fluentd-elasticsearch</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">tolerations:</span></span><br><span class="line">      <span class="comment"># this toleration is to have the daemonset runnable on master nodes</span></span><br><span class="line">      <span class="comment"># remove it if your masters can't run pods</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">node-role.kubernetes.io/master</span></span><br><span class="line">        <span class="attr">effect:</span> <span class="string">NoSchedule</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">fluentd-elasticsearch</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">quay.io/fluentd_elasticsearch/fluentd:v2.5.2</span></span><br><span class="line">        <span class="attr">resources:</span></span><br><span class="line">          <span class="attr">limits:</span></span><br><span class="line">            <span class="attr">memory:</span> <span class="string">200Mi</span></span><br><span class="line">          <span class="attr">requests:</span></span><br><span class="line">            <span class="attr">cpu:</span> <span class="string">100m</span></span><br><span class="line">            <span class="attr">memory:</span> <span class="string">200Mi</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">varlog</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/var/log</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">varlibdockercontainers</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/var/lib/docker/containers</span></span><br><span class="line">          <span class="attr">readOnly:</span> <span class="literal">true</span></span><br><span class="line">      <span class="attr">terminationGracePeriodSeconds:</span> <span class="number">30</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">varlog</span></span><br><span class="line">        <span class="attr">hostPath:</span></span><br><span class="line">          <span class="attr">path:</span> <span class="string">/var/log</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">varlibdockercontainers</span></span><br><span class="line">        <span class="attr">hostPath:</span></span><br><span class="line">          <span class="attr">path:</span> <span class="string">/var/lib/docker/containers</span></span><br></pre></td></tr></table></figure><p>使用 <strong>yaml</strong> 文件创建 <strong>DaemonSet</strong>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://k8s.io/examples/controllers/daemonset.yaml</span><br></pre></td></tr></table></figure><h2 id="如何调度-Daemon-Pods"><a href="#如何调度-Daemon-Pods" class="headerlink" title="如何调度 Daemon Pods"></a>如何调度 Daemon Pods</h2><h3 id="通过默认调度器调度"><a href="#通过默认调度器调度" class="headerlink" title="通过默认调度器调度"></a>通过默认调度器调度</h3><p>DaemonSet 确保所有符合条件的节点都运行该 Pod 的一个副本。 通常，运行 Pod 的节点由 Kubernetes 调度器选择。不过，DaemonSet pods 由 DaemonSet 控制器创建和调度。这就带来了以下问题:</p><ul><li>Pod 行为的不一致性: 正常 Pod 在被创建后等待调度时处于 Pending 状态， DaemonSet Pods 创建后不会处于 Pending 状态下。这使用户感到困惑。</li><li><a href="https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/" target="_blank" rel="noopener">Pod 抢占</a> 由默认调度器处理。启用抢占后，DaemonSet 控制器将在不考虑 Pod 优先级和抢占 的情况下制定调度决策。</li></ul><p><strong>ScheduleDaemonSetPods</strong> 控制 Kubernetes 使用 <strong>默认调度器</strong> 而不是 <strong>DaemonSet 控制器</strong> 来调度 DaemonSets，通过将 <strong>yaml</strong> 配置文件中 <strong>Pod</strong> 部分的 <strong>.spec.nodeName</strong> 替换为 <strong>.spec.affinity.nodeAffinity</strong>。更多内容请点击 <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity" target="_blank" rel="noopener">Assigning Pods to Nodes: Affinity and anti-affinity</a>。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">with-node-affinity</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">affinity:</span></span><br><span class="line">    <span class="attr">nodeAffinity:</span></span><br><span class="line">      <span class="attr">requiredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">        <span class="attr">nodeSelectorTerms:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">matchExpressions:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">kubernetes.io/e2e-az-name</span></span><br><span class="line">            <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">            <span class="attr">values:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">e2e-az1</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">e2e-az2</span></span><br><span class="line">      <span class="attr">preferredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">weight:</span> <span class="number">1</span></span><br><span class="line">        <span class="attr">preference:</span></span><br><span class="line">          <span class="attr">matchExpressions:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">another-node-label-key</span></span><br><span class="line">            <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">            <span class="attr">values:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">another-node-label-value</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">with-node-affinity</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">k8s.gcr.io/pause:2.0</span></span><br></pre></td></tr></table></figure><ul><li><strong>requiredDuringSchedulingIgnoredDuringExecution</strong>: 必须将 Pod 部署到满足条件的节点上，否则不断重试</li><li><strong>preferredDuringSchedulingIgnoredDuringExecution</strong>: 优先将 Pod 部署到满足条件的节点上，否则忽略该条件</li></ul><p>此外，系统会自动添加 <strong>node.kubernetes.io/unschedulable: NoSchedule</strong> 容忍度到 <strong>DaemonSet Pods</strong>。在调度 DaemonSet Pod 时，默认调度器会忽略 <strong>unschedulable</strong> 节点。</p><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/" target="_blank" rel="noopener">Kubernetes DaemonSet</a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/" target="_blank" rel="noopener">Pod Priority and Preemption</a></li><li><a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity" target="_blank" rel="noopener">Assigning Pods to Nodes: Affinity and anti-affinity</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;DaemonSet&lt;/strong&gt; 确保全部 (或者某些) 节点上运行一个 Pod 的副本。 当有节点加入集群时，也会为他们新增一个 Pod 。当有节点从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除它创建的所有 Pod。&lt;/p&gt;</summary>
      
    
    
    
    
    <category term="kubernetes" scheme="http://divinerapier.github.io/tags/kubernetes/"/>
    
    <category term="controllers" scheme="http://divinerapier.github.io/tags/controllers/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes StatefulSet</title>
    <link href="http://divinerapier.github.io/2020/10/27/kubernetes-statefulset/"/>
    <id>http://divinerapier.github.io/2020/10/27/kubernetes-statefulset/</id>
    <published>2020-10-27T03:58:31.000Z</published>
    <updated>2020-10-27T07:30:40.598Z</updated>
    
    <content type="html"><![CDATA[<p>StatefulSet 是用来管理有状态应用的工作负载 API 对象。</p><p>StatefulSet 用来管理 Deployment 和扩展一组 Pod，并且能为这些 Pod 提供序号和唯一性保证。</p><p>和 Deployment 相同的是，StatefulSet 管理了基于相同容器定义的一组 Pod。但和 Deployment 不同的是，StatefulSet 为它们的每个 Pod 维护了一个固定的 ID。这些 Pod 是基于相同的声明来创建的，但是不能相互替换：无论怎么调度，每个 Pod 都有一个永久不变的 ID。</p><p>StatefulSet 和其他控制器使用相同的工作模式。你在 StatefulSet 对象 中定义你期望的状态，然后 StatefulSet 的 控制器 就会通过各种更新来达到那种你想要的状态。</p><h2 id="使用-StatefulSets"><a href="#使用-StatefulSets" class="headerlink" title="使用 StatefulSets"></a>使用 StatefulSets</h2><p>StatefulSets 对于需要满足以下一个或多个需求的应用程序很有价值:</p><ul><li>稳定的、唯一的网络标识符。</li><li>稳定的、持久的存储。</li><li>有序的、优雅的部署和缩放。</li><li>有序的、自动的滚动更新。</li></ul><p>在上面，稳定意味着 Pod 调度或重调度的整个过程是有持久性的。如果应用程序不需要任何稳定的标识符或有序的部署、删除或伸缩，则应该使用由一组无状态的副本控制器提供的工作负载来部署应用程序，比如 Deployment 或者 ReplicaSet 可能更适用于您的无状态应用部署需要。</p><h2 id="限制"><a href="#限制" class="headerlink" title="限制"></a>限制</h2><ul><li>给定 Pod 的存储必须由 <a href="https://github.com/kubernetes/examples/tree/master/staging/persistent-volume-provisioning/README.md" target="_blank" rel="noopener">PersistentVolume</a> 驱动 基于所请求的 <strong>storage class</strong> 来提供，或者由管理员预先提供。</li><li>删除或者收缩 StatefulSet 并 <strong>不会删除</strong> 它关联的存储卷。这样做是为了保证数据安全，它通常比自动清除 StatefulSet 所有相关的资源更有价值。</li><li>StatefulSet 当前需要 <a href="https://kubernetes.io/docs/concepts/services-networking/service/#headless-services" target="_blank" rel="noopener">Headless Services</a> 来负责 Pod 的网络标识。用户需要负责创建此服务。</li><li>当删除 StatefulSets 时，StatefulSet 不提供任何终止 Pod 的保证。为了实现 StatefulSet 中的 Pod 可以有序和优雅的终止，可以在删除之前将 StatefulSet 缩放为 0。</li><li>在默认 <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies" target="_blank" rel="noopener">Pod 管理策略</a>(<strong>OrderedReady</strong>) 时使用 <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#rolling-updates" target="_blank" rel="noopener">滚动更新</a>，可能进入需要 <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#forced-rollback" target="_blank" rel="noopener">人工干预</a> 才能修复的损坏状态。</li></ul><h2 id="组件"><a href="#组件" class="headerlink" title="组件"></a>组件</h2><p>下面的示例演示了 StatefulSet 的组件。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">web</span></span><br><span class="line">  <span class="attr">clusterIP:</span> <span class="string">None</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StatefulSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">web</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">nginx</span> <span class="comment"># has to match .spec.template.metadata.labels</span></span><br><span class="line">  <span class="attr">serviceName:</span> <span class="string">"nginx"</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span> <span class="comment"># by default is 1</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">nginx</span> <span class="comment"># has to match .spec.selector.matchLabels</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">terminationGracePeriodSeconds:</span> <span class="number">10</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">k8s.gcr.io/nginx-slim:0.8</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">web</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">www</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/usr/share/nginx/html</span></span><br><span class="line">  <span class="attr">volumeClaimTemplates:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">www</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">accessModes:</span> <span class="string">[</span> <span class="string">"ReadWriteOnce"</span> <span class="string">]</span></span><br><span class="line">      <span class="attr">storageClassName:</span> <span class="string">"my-storage-class"</span></span><br><span class="line">      <span class="attr">resources:</span></span><br><span class="line">        <span class="attr">requests:</span></span><br><span class="line">          <span class="attr">storage:</span> <span class="string">1Gi</span></span><br></pre></td></tr></table></figure><ul><li>名为 <strong>nginx</strong> 的 Headless Service 用来控制网络域名。</li><li>名为 <strong>web</strong> 的 StatefulSet 有一个 Spec，它表明将在独立的 3 个 Pod 副本中启动 nginx 容器。</li><li><strong>volumeClaimTemplates</strong> 将通过 PersistentVolumes 驱动提供的 <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/" target="_blank" rel="noopener">PersistentVolumes</a> 来提供稳定的存储。</li></ul><p>StatefulSet 对象的 <strong>name</strong> 必须是合法的 <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/names#dns-subdomain-names" target="_blank" rel="noopener">DNS 域名</a>。</p><h2 id="Pod-Selector"><a href="#Pod-Selector" class="headerlink" title="Pod Selector"></a>Pod Selector</h2><p>必须将 StatefullSet 的 <strong>.spec.selector</strong> 字段与 <strong>.spec.template.metadata.labels</strong> 设置相同的值。</p><p>在 Kubernetes 1.8 版本之前，忽略 <strong>.spec.selector</strong> 字段会获得默认设置值。在 1.8 及以后的版本中，未指定匹配的 Pod Selector 将在创建 StatefulSet 期间导致验证错误。</p><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/" target="_blank" rel="noopener">Kubernetes StatefulSet</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;StatefulSet 是用来管理有状态应用的工作负载 API 对象。&lt;/p&gt;
&lt;p&gt;StatefulSet 用来管理 Deployment 和扩展一组 Pod，并且能为这些 Pod 提供序号和唯一性保证。&lt;/p&gt;
&lt;p&gt;和 Deployment 相同的是，Stateful</summary>
      
    
    
    
    
    <category term="kubernetes" scheme="http://divinerapier.github.io/tags/kubernetes/"/>
    
    <category term="controllers" scheme="http://divinerapier.github.io/tags/controllers/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes Deployment</title>
    <link href="http://divinerapier.github.io/2020/10/27/kubernetes-deployment/"/>
    <id>http://divinerapier.github.io/2020/10/27/kubernetes-deployment/</id>
    <published>2020-10-27T02:48:23.000Z</published>
    <updated>2020-10-27T06:43:19.458Z</updated>
    
    <content type="html"><![CDATA[<p>一个 Deployment 控制器为 <a href="https://kubernetes.io/docs/concepts/workloads/pods/" target="_blank" rel="noopener">Pods</a> 和 <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/" target="_blank" rel="noopener">ReplicaSets</a> 提供声明式的更新能力。</p><p>用户负责描述 Deployment 中的 <strong>目标状态</strong>，而 Deployment 控制器以受控速率更改 <strong>实际状态</strong>，使其变为 <strong>期望状态</strong>。用户可以定义 Deployment 以创建新的 ReplicaSet，或删除现有 Deployment， 并通过新的 Deployment 接收(adopt)其资源。</p><blockquote><p><strong>说明</strong>： 不要管理 Deployment 所拥有的 ReplicaSet 。 如果存在下面未覆盖的使用场景，请考虑在 Kubernetes 仓库中提出 Issue。</p></blockquote><h2 id="用例"><a href="#用例" class="headerlink" title="用例"></a>用例</h2><p>以下是 Deployments 的典型用例：</p><ul><li><a href="#Deployment">创建 Deployment 使 ReplicaSet 上线(rollout)</a>。 ReplicaSet 在后台创建 Pods。 检查 ReplicaSet 的上线状态，查看其是否成功。</li><li>通过更新 Deployment 的 PodTemplateSpec，声明 Pod 的新状态 。 新的 ReplicaSet 会被创建，Deployment 以受控速率将 Pod 从旧 ReplicaSet 迁移到新 ReplicaSet。 每个新的 ReplicaSet 都会更新 Deployment 的修订版本。</li><li>如果 Deployment 的当前状态不稳定，回滚到较早的 Deployment 版本。 每次回滚都会更新 Deployment 的修订版本。</li><li>扩大 Deployment 规模以承担更多负载。</li><li>暂停 Deployment 以应用对 PodTemplateSpec 所作的多项修改， 然后恢复其执行以启动新的上线版本。</li><li>使用 Deployment 状态 来判定上线过程是否出现停滞。</li><li>清理较旧的不再需要的 ReplicaSet。</li></ul><h2 id="创建-Deployment"><a href="#创建-Deployment" class="headerlink" title="创建 Deployment"></a>创建 Deployment</h2><p>下面是 Deployment 示例。Deployment 创建一个 ReplicaSet，负责启动三个 <strong>nginx</strong> Pods:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-deployment</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">nginx:1.14.2</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure><p>在该例中:</p><ul><li><p>创建名为 <strong>nginx-deployment</strong> (由 <strong>.metadata.name</strong> 字段标明) 的 Deployment。</p></li><li><p>该 Deployment 创建三个 (由 <strong>.spec.replicas</strong> 字段标明) Pod 副本。</p></li><li><p><strong>.spec.selector</strong> 字段定义 Deployment 如何查找要管理的 Pods。 在这里，你只需选择在 Pod 模板中定义的标签（app: nginx）。 不过，更复杂的选择规则是也可能的，只要 Pod 模板</p><blockquote><p><strong>说明</strong>： <strong>matchLabels</strong> 字段是 {key,value} 字典映射。在 <strong>matchLabels</strong> 映射中的单个 {key,value} 映射等效于 <strong>matchExpressions</strong> 中的一个元素，即其 key 字段是 “key”，operator 为 “In”，value 数组仅包含 “value”。在 <strong>matchLabels</strong> 和 <strong>matchExpressions</strong> 中给出的所有条件都必须满足才能匹配。</p></blockquote></li><li><p><strong>.spec.template</strong> 字段包含以下子字段:</p><ul><li>使用 <strong>.metadata.labels</strong> 字段为 Pod 设置标签 <strong>app: nginx</strong></li><li><strong>.template.spec</strong> 字段表示 Pod 的模板，指示 Pods 运行一个 <strong>nginx</strong> 容器，该容器运行 <strong>nginx:1.14.2</strong> 镜像。</li><li>创建一个容器，使用 <strong>.spec.template.spec.containers[0].name</strong> 字段 <strong>nginx</strong> 作为名字</li></ul></li></ul><p>开始之前，请确保的 Kubernetes 集群已启动并运行。 按照以下步骤创建上述 Deployment:</p><ol><li><p>通过运行以下命令创建 Deployment:</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://k8s.io/examples/controllers/nginx-deployment.yaml</span><br></pre></td></tr></table></figure></li><li><p>检查 Deployment 是否已创建。如果仍在创建 Deployment， 则输出类似于:</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kubectl get deployments</span><br><span class="line"></span><br><span class="line">NAME               READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">nginx-deployment   0/3     0            0           1s</span><br></pre></td></tr></table></figure><p> 在检查集群中的 Deployment 时，所显示的字段有：</p><ul><li>NAME 列出了集群中 Deployment 的名称。</li><li>READY 显示应用程序的可用的 <strong>副本</strong> 数。显示的模式是“就绪个数/期望个数”。</li><li>UP-TO-DATE 显示为了打到期望状态已经更新的副本数。</li><li>AVAILABLE 显示应用可供用户使用的副本数。</li><li>AGE 显示应用程序运行的时间。<br>请注意期望副本数是根据 <strong>.spec.replicas</strong> 字段设置 3。</li></ul></li><li><p>查看 Deployment 上线状态:</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kubectl rollout status deployment.v1.apps/nginx-deployment</span><br><span class="line"></span><br><span class="line">Waiting <span class="keyword">for</span> rollout to finish: 2 out of 3 new replicas have been updated...</span><br><span class="line">deployment <span class="string">"nginx-deployment"</span> successfully rolled out</span><br></pre></td></tr></table></figure></li><li><p>查看 Deployment 创建的 ReplicaSet (rs):</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kubectl get rs</span><br><span class="line"></span><br><span class="line">NAME                          DESIRED   CURRENT   READY   AGE</span><br><span class="line">nginx-deployment-75675f5897   3         3         3       18s</span><br></pre></td></tr></table></figure><p> ReplicaSet 输出中包含以下字段:</p><ul><li>NAME 列出名字空间中 ReplicaSet 的名称；</li><li>DESIRED 显示应用的期望副本个数，即在创建 Deployment 时所定义的值。 此为期望状态；</li><li>CURRENT 显示当前运行状态中的副本个数；</li><li>READY 显示应用中有多少副本可以为用户提供服务；</li><li>AGE 显示应用已经运行的时间长度。<blockquote><p><strong>注意</strong>: ReplicaSet 的名称始终被格式化为 <strong>[Deployment名称]-[随机字符串]</strong>。 其中的随机字符串是使用 <strong>pod-template-hash</strong> 作为种子随机生成的。</p></blockquote></li></ul></li><li><p>查看每个 Pod 自动生成的标签:</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods --show-labels</span><br><span class="line"></span><br><span class="line">NAME                                READY     STATUS    RESTARTS   AGE       LABELS</span><br><span class="line">nginx-deployment-75675f5897-7ci7o   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453</span><br><span class="line">nginx-deployment-75675f5897-kzszj   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453</span><br><span class="line">nginx-deployment-75675f5897-qqcnn   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453</span><br></pre></td></tr></table></figure><p> 所创建的 ReplicaSet 确保总是存在三个 <strong>nginx</strong> Pod。</p></li></ol><blockquote><p><strong>说明</strong>: 必须在 Deployment 中指定适当的 <strong>.spec.selector</strong> 和 <strong>.spec.template.metadata.labels</strong>。不要与其他控制器重叠。 Kubernetes 不会阻止这样做，但是如果多个控制器具有重叠的 selector，它们可能会发生冲突 执行难以预料的操作。</p></blockquote><h3 id="Pod-template-hash-标签"><a href="#Pod-template-hash-标签" class="headerlink" title="Pod-template-hash 标签"></a>Pod-template-hash 标签</h3><blockquote><p><strong>注意</strong>: 不要更改此标签</p></blockquote><p>Deployment 控制器将 <strong>pod-template-hash</strong> 标签添加到 Deployment 所创建或接收(adopt) 的 每个 ReplicaSet 。</p><p>此标签可确保 Deployment 的子 ReplicaSets 不重叠。 标签是通过对 ReplicaSet 的 PodTemplate 进行哈希处理。 所生成的哈希值被添加到 ReplicaSet 的 <strong>selector</strong>、Pod 的 <strong>label</strong>，并存在于在 ReplicaSet 可能拥有的任何现有 Pod 中。</p><h2 id="更新-Deployment"><a href="#更新-Deployment" class="headerlink" title="更新 Deployment"></a>更新 Deployment</h2><blockquote><p><strong>说明</strong>: 仅当 Deployment 的 <strong>.spec.template</strong> 发生改变时，例如模板的标签或容器镜像被更新，才会触发 Deployment 上线。其他更新(如对 Deployment 执行扩缩容的操作) 不会触发上线动作。</p></blockquote><p>按照以下步骤更新 Deployment:</p><ol><li><p>更新 <strong>nginx</strong> Pod 镜像，从 <strong>nginx:1.14.2</strong> 到 <strong>nginx:1.16.1</strong>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl --record deployment.apps/nginx-deployment <span class="built_in">set</span> image \</span><br><span class="line">   deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1</span><br><span class="line"></span><br><span class="line">deployment.apps/nginx-deployment image updated</span><br></pre></td></tr></table></figure><p>或者使用下面的命令:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl <span class="built_in">set</span> image deployment/nginx-deployment nginx=nginx:1.16.1 --record</span><br><span class="line"></span><br><span class="line">deployment.apps/nginx-deployment image updated</span><br></pre></td></tr></table></figure><p>或者，可以 <strong>edit</strong> Deployment 并将 <strong>.spec.template.spec.containers[0].image</strong> 从 <strong>nginx:1.14.2</strong> 更改至 <strong>nginx:1.16.1</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl edit deployment.v1.apps/nginx-deployment</span><br><span class="line"></span><br><span class="line">deployment.apps/nginx-deployment edited</span><br></pre></td></tr></table></figure></li><li><p>查看上线状态，运行:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl rollout status deployment.v1.apps/nginx-deployment</span><br><span class="line"></span><br><span class="line">Waiting <span class="keyword">for</span> rollout to finish: 2 out of 3 new replicas have been updated...</span><br><span class="line"></span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line"></span><br><span class="line">deployment <span class="string">"nginx-deployment"</span> successfully rolled out</span><br></pre></td></tr></table></figure></li></ol><p>获取关于已更新的 Deployment 的更多信息:</p><ul><li><p>查看 Deployment:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get deployments</span><br><span class="line"></span><br><span class="line">NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">nginx-deployment   3         3         3            3           36s</span><br></pre></td></tr></table></figure></li><li><p>查看 Deployment 通过创建新的 ReplicaSet:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get rs</span><br><span class="line"></span><br><span class="line">NAME                          DESIRED   CURRENT   READY   AGE</span><br><span class="line">nginx-deployment-1564180365   3         3         3       6s</span><br><span class="line">nginx-deployment-2035384211   0         0         0       36s</span><br></pre></td></tr></table></figure></li><li><p>查看 Deployment 的 Pod</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods</span><br><span class="line"></span><br><span class="line">NAME                                READY     STATUS    RESTARTS   AGE</span><br><span class="line">nginx-deployment-1564180365-khku8   1/1       Running   0          14s</span><br><span class="line">nginx-deployment-1564180365-nacti   1/1       Running   0          14s</span><br><span class="line">nginx-deployment-1564180365-z9gth   1/1       Running   0          14s</span><br></pre></td></tr></table></figure><p>Deployment 可确保在更新时仅关闭一定数量的 Pod。默认情况下，它确保至少所需 Pods 数量的 <strong>75%</strong> 处于运行状态 (最大不可用比例为 25%)。</p><p>Deployment 还确保所创建 Pod 的数量只比期望 Pods 的数量超出一定数值。默认情况下，Deployment 可确保实际启动的 Pod 个数最大为期望值的 125%。</p></li><li><p>获取 Deployment 的更多信息:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe deployments</span><br></pre></td></tr></table></figure></li></ul><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/" target="_blank" rel="noopener">Kubernetes Pods</a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/" target="_blank" rel="noopener">Kubernetes ReplicaSets</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;一个 Deployment 控制器为 &lt;a href=&quot;https://kubernetes.io/docs/concepts/workloads/pods/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Pods&lt;/a&gt; 和 &lt;a href=&quot;https</summary>
      
    
    
    
    
    <category term="kubernetes" scheme="http://divinerapier.github.io/tags/kubernetes/"/>
    
    <category term="controllers" scheme="http://divinerapier.github.io/tags/controllers/"/>
    
  </entry>
  
  <entry>
    <title>/opt/cni - readonly filesystem</title>
    <link href="http://divinerapier.github.io/2020/10/26/opt-cni-is-readonly-directory/"/>
    <id>http://divinerapier.github.io/2020/10/26/opt-cni-is-readonly-directory/</id>
    <published>2020-10-26T03:44:37.000Z</published>
    <updated>2020-10-26T04:21:08.386Z</updated>
    
    <content type="html"><![CDATA[<p>在部署 <a href="https://docs.projectcalico.org/getting-started/kubernetes/flannel/flannel" target="_blank" rel="noopener">Canal</a> 时遇到如下错误:</p><ol><li><p>查看 <strong>Pod</strong> 状态:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods --all-namespaces</span><br><span class="line"></span><br><span class="line">NAMESPACE     NAME                                                              READY   STATUS                   RESTARTS   AGE</span><br><span class="line">kube-system   canal-5qk26                                                       0/2     Init:RunContainerError   0          10m</span><br><span class="line">kube-system   kube-proxy-tt2qn                                                  0/1     CrashLoopBackOff         10         11m</span><br></pre></td></tr></table></figure></li><li><p>查看 <strong>canal-5qk26</strong> 事件:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl -n kube-system describe pod canal-5qk26</span><br><span class="line"></span><br><span class="line">Events:</span><br><span class="line">  Type     Reason          Age                     From                Message</span><br><span class="line">  ----     ------          ----                    ----                -------</span><br><span class="line">  Normal   Scheduled       6m41s                   default-scheduler   Successfully assigned kube-system/canal-5qk26 to ubuntu-01</span><br><span class="line">  Warning  Failed          6m40s                   kubelet, ubuntu-01  Error: failed to start container <span class="string">"install-cni"</span>: Error response from daemon: can<span class="string">'t join IPC of container 1f3affaa9eba3f1087ac2309f7c4147a54e2cbad09be733e9c394ce7a8ba583b: container 1f3affaa9eba3f1087ac2309f7c4147a54e2cbad09be733e9c394ce7a8ba583b is not running</span></span><br><span class="line"><span class="string">  Warning  Failed          6m39s                   kubelet, ubuntu-01  Error: failed to start container "install-cni": Error response from daemon: cannot join network of a non running container: 1861c340dfadba101b333af1163329a88a8e02fd25e5d657b4e0954acd09d3d5</span></span><br><span class="line"><span class="string">  Warning  Failed          6m37s                   kubelet, ubuntu-01  Error: failed to start container "install-cni": Error response from daemon: error while creating mount source path '</span>/opt/cni/bin<span class="string">': mkdir /opt/cni: read-only file system</span></span><br><span class="line"><span class="string">  Warning  BackOff         6m34s (x3 over 6m38s)   kubelet, ubuntu-01  Back-off restarting failed container</span></span><br><span class="line"><span class="string">  Normal   Pulled          6m33s (x5 over 6m40s)   kubelet, ubuntu-01  Container image "calico/cni:v3.16.4" already present on machine</span></span><br><span class="line"><span class="string">  Normal   Created         6m33s (x5 over 6m40s)   kubelet, ubuntu-01  Created container install-cni</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Normal   SandboxChanged  100s (x269 over 6m39s)  kubelet, ubuntu-01  Pod sandbox changed, it will be killed and re-created.</span></span><br></pre></td></tr></table></figure></li><li><p>查看 <strong>kube-proxy-tt2qn</strong> 事件:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl -n kube-system describe pod kube-proxy-tt2qn</span><br><span class="line"></span><br><span class="line">Events:</span><br><span class="line">  Type     Reason          Age                      From                Message</span><br><span class="line">  ----     ------          ----                     ----                -------</span><br><span class="line">  Normal   Scheduled       11m                      default-scheduler   Successfully assigned kube-system/kube-proxy-tt2qn to ubuntu-01</span><br><span class="line">  Normal   Pulled          11m (x2 over 11m)        kubelet, ubuntu-01  Container image <span class="string">"registry.aliyuncs.com/google_containers/kube-proxy:v1.19.3"</span> already present on machine</span><br><span class="line">  Normal   Created         11m (x2 over 11m)        kubelet, ubuntu-01  Created container kube-proxy</span><br><span class="line">  Warning  Failed          11m                      kubelet, ubuntu-01  Error: failed to start container <span class="string">"kube-proxy"</span>: Error response from daemon: cannot join network of a non running container: 119388927b173ad23226c1049db0c1269ada343b133774807a1615cc79442246</span><br><span class="line">  Warning  BackOff         11m (x9 over 11m)        kubelet, ubuntu-01  Back-off restarting failed container</span><br><span class="line">  Normal   SandboxChanged  11m (x10 over 11m)       kubelet, ubuntu-01  Pod sandbox changed, it will be killed and re-created.</span><br></pre></td></tr></table></figure></li><li><p>查看 <strong>Worker</strong> 节点上 <strong>Kubelet</strong> 与 <strong>Docker</strong> 日志:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ journalctl -f</span><br><span class="line"></span><br><span class="line">  -- Logs begin at Mon 2020-10-19 10:09:50 UTC. --</span><br><span class="line">  Oct 26 02:31:35 ubuntu-01 docker.dockerd[1781]: time=<span class="string">"2020-10-26T02:31:35.429587085Z"</span> level=error msg=<span class="string">"Handler for POST /v1.40/containers/aef879b89cd75d23249e76091a4716c1303855b904979e983920aae02da01d18/start returned error: error while creating mount source path '/opt/cni/bin': mkdir /opt/cni: read-only file system"</span></span><br><span class="line">  Oct 26 02:31:35 ubuntu-01 kubelet[281110]: E1026 02:31:35.474035  281110 remote_runtime.go:248] StartContainer <span class="string">"aef879b89cd75d23249e76091a4716c1303855b904979e983920aae02da01d18"</span> from runtime service failed: rpc error: code = Unknown desc = failed to start container <span class="string">"aef879b89cd75d23249e76091a4716c1303855b904979e983920aae02da01d18"</span>: Error response from daemon: error <span class="keyword">while</span> creating mount <span class="built_in">source</span> path <span class="string">'/opt/cni/bin'</span>: mkdir /opt/cni: <span class="built_in">read</span>-only file system</span><br><span class="line">  Oct 26 02:31:35 ubuntu-01 kubelet[281110]: E1026 02:31:35.474175  281110 pod_workers.go:191] Error syncing pod 5f45d450-c4e9-45dc-b2c6-52a95570ba71 (<span class="string">"canal-5qk26_kube-system(5f45d450-c4e9-45dc-b2c6-52a95570ba71)"</span>), skipping: failed to <span class="string">"StartContainer"</span> <span class="keyword">for</span> <span class="string">"install-cni"</span> with RunContainerError: <span class="string">"failed to start container \"aef879b89cd75d23249e76091a4716c1303855b904979e983920aae02da01d18\": Error response from daemon: error while creating mount source path '/opt/cni/bin': mkdir /opt/cni: read-only file system"</span></span><br><span class="line">  Oct 26 02:31:36 ubuntu-01 audit[315954]: AVC apparmor=<span class="string">"DENIED"</span> operation=<span class="string">"exec"</span> info=<span class="string">"no new privs"</span> error=-1 profile=<span class="string">"snap.docker.dockerd"</span> name=<span class="string">"/pause"</span> pid=315954 comm=<span class="string">"runc:[2:INIT]"</span> requested_mask=<span class="string">"x"</span> denied_mask=<span class="string">"x"</span> fsuid=0 ouid=0 target=<span class="string">"docker-default"</span></span><br><span class="line">  Oct 26 02:31:36 ubuntu-01 kernel: audit: <span class="built_in">type</span>=1400 audit(1603679496.465:6039): apparmor=<span class="string">"DENIED"</span> operation=<span class="string">"exec"</span> info=<span class="string">"no new privs"</span> error=-1 profile=<span class="string">"snap.docker.dockerd"</span> name=<span class="string">"/pause"</span> pid=315954 comm=<span class="string">"runc:[2:INIT]"</span> requested_mask=<span class="string">"x"</span> denied_mask=<span class="string">"x"</span> fsuid=0 ouid=0 target=<span class="string">"docker-default"</span></span><br><span class="line">  Oct 26 02:31:36 ubuntu-01 kubelet[281110]: W1026 02:31:36.635730  281110 cni.go:239] Unable to update cni config: no networks found <span class="keyword">in</span> /etc/cni/net.d</span><br></pre></td></tr></table></figure></li></ol><p>根据上述日志信息，可以确认根本错误出现在 <strong>Docker</strong> 中:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">error while creating mount source path &#39;&#x2F;opt&#x2F;cni&#x2F;bin&#39;: mkdir &#x2F;opt&#x2F;cni: read-only file system</span><br></pre></td></tr></table></figure><p>但宿主机目录 <strong>/opt/cni</strong> 的权限为:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">stat</span> /opt/cni/</span><br><span class="line"></span><br><span class="line">  File: /opt/cni/</span><br><span class="line">  Size: 4096            Blocks: 8          IO Block: 4096   directory</span><br><span class="line">Device: fd00h/64768d    Inode: 917533      Links: 3</span><br><span class="line">Access: (0755/drwxr-xr-x)  Uid: (    0/    root)   Gid: (    0/    root)</span><br><span class="line">Access: 2020-10-26 01:41:07.222221020 +0000</span><br><span class="line">Modify: 2020-10-20 13:00:36.461719609 +0000</span><br><span class="line">Change: 2020-10-20 13:00:36.461719609 +0000</span><br><span class="line"> Birth: -</span><br></pre></td></tr></table></figure><p>因此，推测为 <strong>Docker</strong> 服务的问题。最终，通过重启 <strong>Docker</strong> 解决问题:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl restart docker.service</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在部署 &lt;a href=&quot;https://docs.projectcalico.org/getting-started/kubernetes/flannel/flannel&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Canal&lt;/a&gt; 时遇到如下错误:</summary>
      
    
    
    
    
    <category term="kubernetes" scheme="http://divinerapier.github.io/tags/kubernetes/"/>
    
    <category term="faq" scheme="http://divinerapier.github.io/tags/faq/"/>
    
  </entry>
  
  <entry>
    <title>MPI Operator</title>
    <link href="http://divinerapier.github.io/2020/10/24/mpi-operator/"/>
    <id>http://divinerapier.github.io/2020/10/24/mpi-operator/</id>
    <published>2020-10-24T10:17:12.000Z</published>
    <updated>2020-10-24T14:50:26.760Z</updated>
    
    <content type="html"><![CDATA[<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>部署默认配置的 <strong>mpi-operator</strong>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/kubeflow/mpi-operator</span><br><span class="line"><span class="built_in">cd</span> mpi-operator</span><br><span class="line">kubectl create -f deploy/v1alpha2/mpi-operator.yaml</span><br></pre></td></tr></table></figure><p>验证是否安装成功:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl get crd</span><br><span class="line"><span class="comment"># NAME                                          CREATED AT</span></span><br><span class="line"><span class="comment"># mpijobs.kubeflow.org                          2020-10-23T08:40:15Z</span></span><br></pre></td></tr></table></figure><h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p>创建一个 <strong>MPIJob</strong> 的配置文件:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">kubeflow.org/v1alpha2</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">MPIJob</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">openmpi-helloworld</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">slotsPerWorker:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">cleanPodPolicy:</span> <span class="string">Running</span></span><br><span class="line">  <span class="attr">mpiReplicaSpecs:</span></span><br><span class="line">    <span class="attr">Launcher:</span></span><br><span class="line">      <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">      <span class="attr">template:</span></span><br><span class="line">         <span class="attr">spec:</span></span><br><span class="line">           <span class="attr">containers:</span></span><br><span class="line">           <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">divinerapier/openmpi-helloworld:0.0.1</span></span><br><span class="line">             <span class="attr">name:</span> <span class="string">openmpi-helloworld</span></span><br><span class="line">             <span class="attr">command:</span></span><br><span class="line">             <span class="bullet">-</span> <span class="string">mpirun</span></span><br><span class="line">             <span class="bullet">-</span> <span class="string">--allow-run-as-root</span></span><br><span class="line">             <span class="bullet">-</span> <span class="string">-np</span></span><br><span class="line">             <span class="bullet">-</span> <span class="string">"2"</span></span><br><span class="line">             <span class="bullet">-</span> <span class="string">/helloworld/mpi_hello_world</span></span><br><span class="line">             <span class="attr">resources:</span></span><br><span class="line">               <span class="attr">limits:</span></span><br><span class="line">                 <span class="attr">cpu:</span> <span class="string">10Mi</span></span><br><span class="line">                 <span class="attr">memory:</span> <span class="string">10Mi</span></span><br><span class="line">    <span class="attr">Worker:</span></span><br><span class="line">      <span class="attr">replicas:</span> <span class="number">2</span></span><br><span class="line">      <span class="attr">template:</span></span><br><span class="line">        <span class="attr">spec:</span></span><br><span class="line">          <span class="attr">containers:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">divinerapier/openmpi-helloworld:0.0.1</span></span><br><span class="line">            <span class="attr">name:</span> <span class="string">openmpi-helloworld</span></span><br><span class="line">            <span class="attr">resources:</span></span><br><span class="line">              <span class="attr">limits:</span></span><br><span class="line">                <span class="attr">cpu:</span> <span class="string">10Mi</span></span><br><span class="line">                <span class="attr">memory:</span> <span class="string">10Mi</span></span><br></pre></td></tr></table></figure><p>部署到 <strong>Kubernetes</strong> 上:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f ./openmpi-helloworld.yml</span><br></pre></td></tr></table></figure><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="https://github.com/kubeflow/mpi-operator" target="_blank" rel="noopener">GitHub: MPI Operator</a></li><li><a href="https://medium.com/kubeflow/introduction-to-kubeflow-mpi-operator-and-industry-adoption-296d5f2e6edc" target="_blank" rel="noopener">Introduction to Kubeflow MPI Operator and Industry Adoption</a></li><li><a href="https://www.kubeflow.org/docs/components/training/mpi/" target="_blank" rel="noopener">MPI Training</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;安装&quot;&gt;&lt;a href=&quot;#安装&quot; class=&quot;headerlink&quot; title=&quot;安装&quot;&gt;&lt;/a&gt;安装&lt;/h2&gt;&lt;p&gt;部署默认配置的 &lt;strong&gt;mpi-operator&lt;/strong&gt;:&lt;/p&gt;
&lt;figure class=&quot;highlight ba</summary>
      
    
    
    
    
    <category term="kubernetes" scheme="http://divinerapier.github.io/tags/kubernetes/"/>
    
    <category term="mpi" scheme="http://divinerapier.github.io/tags/mpi/"/>
    
    <category term="operator" scheme="http://divinerapier.github.io/tags/operator/"/>
    
  </entry>
  
  <entry>
    <title>在 Docker 中使用 OpenMPI</title>
    <link href="http://divinerapier.github.io/2020/10/24/openmpi-in-docker/"/>
    <id>http://divinerapier.github.io/2020/10/24/openmpi-in-docker/</id>
    <published>2020-10-24T08:57:06.000Z</published>
    <updated>2020-10-26T04:06:45.546Z</updated>
    
    <content type="html"><![CDATA[<h2 id="构建-Base-镜像"><a href="#构建-Base-镜像" class="headerlink" title="构建 Base 镜像"></a>构建 Base 镜像</h2><p>基于 <strong>Ubuntu 20.04 + OpenMPI 4.0.5</strong> 构建 <strong>Base</strong> 镜像:</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> ubuntu:<span class="number">20.04</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> apt update</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> apt install -y wget gcc g++ make</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Install OpenSSH for MPI to communicate between containers</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> apt-get install -y --no-install-recommends openssh-client openssh-server &amp;&amp; \</span></span><br><span class="line"><span class="bash">    mkdir -p /var/run/sshd</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Allow OpenSSH to talk to containers without asking for confirmation</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> cat /etc/ssh/ssh_config | grep -v StrictHostKeyChecking &gt; /etc/ssh/ssh_config.new &amp;&amp; \</span></span><br><span class="line"><span class="bash">    <span class="built_in">echo</span> <span class="string">"    StrictHostKeyChecking no"</span> &gt;&gt; /etc/ssh/ssh_config.new &amp;&amp; \</span></span><br><span class="line"><span class="bash">    mv /etc/ssh/ssh_config.new /etc/ssh/ssh_config</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Install Open MPI</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> mkdir /tmp/openmpi &amp;&amp; \</span></span><br><span class="line"><span class="bash">    <span class="built_in">cd</span> /tmp/openmpi &amp;&amp; \</span></span><br><span class="line"><span class="bash">    wget https://www.open-mpi.org/software/ompi/v4.0/downloads/openmpi-4.0.5.tar.gz &amp;&amp; \</span></span><br><span class="line"><span class="bash">    tar zxf openmpi-4.0.5.tar.gz &amp;&amp; \</span></span><br><span class="line"><span class="bash">    <span class="built_in">cd</span> openmpi-4.0.5 &amp;&amp; \</span></span><br><span class="line"><span class="bash">    ./configure --<span class="built_in">enable</span>-orterun-prefix-by-default &amp;&amp; \</span></span><br><span class="line"><span class="bash">    make -j $(nproc) all &amp;&amp; \</span></span><br><span class="line"><span class="bash">    make install &amp;&amp; \</span></span><br><span class="line"><span class="bash">    ldconfig</span></span><br></pre></td></tr></table></figure><h3 id="发布镜像"><a href="#发布镜像" class="headerlink" title="发布镜像"></a>发布镜像</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker build -t divinerapier/openmpi:4.0.5 -f=./dockerfile .</span><br><span class="line">docker push divinerapier/openmpi:4.0.5</span><br></pre></td></tr></table></figure><h2 id="应用程序"><a href="#应用程序" class="headerlink" title="应用程序"></a>应用程序</h2><h3 id="程序代码"><a href="#程序代码" class="headerlink" title="程序代码"></a>程序代码</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;mpi.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 初始化 MPI 环境</span></span><br><span class="line">    MPI_Init(<span class="literal">NULL</span>, <span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 通过调用以下方法来得到所有可以工作的进程数量</span></span><br><span class="line">    <span class="keyword">int</span> world_size;</span><br><span class="line">    MPI_Comm_size(MPI_COMM_WORLD, &amp;world_size);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 得到当前进程的秩</span></span><br><span class="line">    <span class="keyword">int</span> world_rank;</span><br><span class="line">    MPI_Comm_rank(MPI_COMM_WORLD, &amp;world_rank);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 得到当前进程的名字</span></span><br><span class="line">    <span class="keyword">char</span> processor_name[MPI_MAX_PROCESSOR_NAME];</span><br><span class="line">    <span class="keyword">int</span> name_len;</span><br><span class="line">    MPI_Get_processor_name(processor_name, &amp;name_len);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印一条带有当前进程名字，秩以及</span></span><br><span class="line">    <span class="comment">// 整个 communicator 的大小的 hello world 消息。</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Hello world from processor %s, rank %d out of %d processors\n"</span>,</span><br><span class="line">           processor_name, world_rank, world_size);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 释放 MPI 的一些资源</span></span><br><span class="line">    MPI_Finalize();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">MPICC ?= mpicc</span><br><span class="line"></span><br><span class="line"><span class="section">all: build</span></span><br><span class="line"></span><br><span class="line"><span class="section">build: main.c</span></span><br><span class="line">    <span class="variable">$(MPICC)</span> -o mpi_hello_world main.c</span><br><span class="line"></span><br><span class="line"><span class="section">clean:</span></span><br><span class="line">    rm -rf mpi_hello_world</span><br></pre></td></tr></table></figure><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> divinerapier/openmpi:<span class="number">4.0</span>.<span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> ./ /helloworld</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> <span class="built_in">cd</span> /helloworld \</span></span><br><span class="line"><span class="bash">  &amp;&amp; make</span></span><br></pre></td></tr></table></figure><h3 id="运行应用程序"><a href="#运行应用程序" class="headerlink" title="运行应用程序"></a>运行应用程序</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker build -t divinerapier/openmpi-helloworld:0.0.1 -f=./dockerfile .</span><br><span class="line">docker run --rm -it divinerapier/openmpi-helloworld:0.0.1 mpirun -allow-run-as-root -np 4 /helloworld/mpi_hello_world</span><br><span class="line"><span class="comment"># Hello world from processor fa74677bda6b, rank 0 out of 4 processors</span></span><br><span class="line"><span class="comment"># Hello world from processor fa74677bda6b, rank 1 out of 4 processors</span></span><br><span class="line"><span class="comment"># Hello world from processor fa74677bda6b, rank 2 out of 4 processors</span></span><br><span class="line"><span class="comment"># Hello world from processor fa74677bda6b, rank 3 out of 4 processors</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;构建-Base-镜像&quot;&gt;&lt;a href=&quot;#构建-Base-镜像&quot; class=&quot;headerlink&quot; title=&quot;构建 Base 镜像&quot;&gt;&lt;/a&gt;构建 Base 镜像&lt;/h2&gt;&lt;p&gt;基于 &lt;strong&gt;Ubuntu 20.04 + OpenMPI 4.0.</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>升级 Ubuntu 发行版</title>
    <link href="http://divinerapier.github.io/2020/10/20/upgrade-ubuntu-release/"/>
    <id>http://divinerapier.github.io/2020/10/20/upgrade-ubuntu-release/</id>
    <published>2020-10-20T08:05:06.000Z</published>
    <updated>2020-10-20T08:15:06.347Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Install all available updates for your release before upgrading.</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt update &amp;&amp; sudo apt upgrade -y</span><br></pre></td></tr></table></figure><p><strong>Reboot system.</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo reboot</span><br></pre></td></tr></table></figure><p><strong>Install the Ubuntu update tool.</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install -y update-manager-core</span><br></pre></td></tr></table></figure><p><strong>Start the upgrade procdure.</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo <span class="keyword">do</span>-release-upgrade</span><br></pre></td></tr></table></figure><p>不建议升级过程通过 <strong>ssh</strong> 连接运行，可以运行在 <strong>tmux</strong> 会话中。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">$ sudo <span class="keyword">do</span>-release-upgrade</span><br><span class="line">Checking <span class="keyword">for</span> a new Ubuntu release</span><br><span class="line">Get:1 Upgrade tool signature [1,554 B]</span><br><span class="line">Get:2 Upgrade tool [1,336 kB]</span><br><span class="line">Fetched 1,338 kB <span class="keyword">in</span> 0s (0 B/s)</span><br><span class="line">authenticate <span class="string">'focal.tar.gz'</span> against <span class="string">'focal.tar.gz.gpg'</span></span><br><span class="line">extracting <span class="string">'focal.tar.gz'</span></span><br><span class="line"></span><br><span class="line">Reading cache</span><br><span class="line"></span><br><span class="line">Checking package manager</span><br><span class="line"></span><br><span class="line">Continue running under SSH?</span><br><span class="line"></span><br><span class="line">This session appears to be running under ssh. It is not recommended</span><br><span class="line">to perform a upgrade over ssh currently because <span class="keyword">in</span> <span class="keyword">case</span> of failure it</span><br><span class="line">is harder to recover.</span><br><span class="line"></span><br><span class="line">If you <span class="built_in">continue</span>, an additional ssh daemon will be started at port</span><br><span class="line"><span class="string">'1022'</span>.</span><br><span class="line">Do you want to <span class="built_in">continue</span>?</span><br><span class="line"></span><br><span class="line">Continue [yN]</span><br></pre></td></tr></table></figure><p><strong>Reboot the box.</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo reboot</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;Install all available updates for your release before upgrading.&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class</summary>
      
    
    
    
    
    <category term="linux" scheme="http://divinerapier.github.io/tags/linux/"/>
    
    <category term="ubuntu" scheme="http://divinerapier.github.io/tags/ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>使用 kubeadm 创建 kubernetes 集群</title>
    <link href="http://divinerapier.github.io/2020/10/19/using-kubeadm-to-create-a-kubernetes-cluster/"/>
    <id>http://divinerapier.github.io/2020/10/19/using-kubeadm-to-create-a-kubernetes-cluster/</id>
    <published>2020-10-19T12:38:25.000Z</published>
    <updated>2020-10-26T04:14:58.976Z</updated>
    
    <content type="html"><![CDATA[<p>在 <strong>Ubuntu 20.04</strong> 系统上搭建 <strong>Kubernetes</strong> 集群。</p><h2 id="安装-Docker"><a href="#安装-Docker" class="headerlink" title="安装 Docker"></a>安装 Docker</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get remove docker docker-engine docker.io containerd runc</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install -y \</span><br><span class="line">    apt-transport-https \</span><br><span class="line">    ca-certificates \</span><br><span class="line">    curl \</span><br><span class="line">    gnupg-agent \</span><br><span class="line">    software-properties-common</span><br><span class="line">curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -</span><br><span class="line">sudo apt-key fingerprint 0EBFCD88</span><br><span class="line">sudo add-apt-repository \</span><br><span class="line">   <span class="string">"deb [arch=amd64] https://download.docker.com/linux/ubuntu \</span></span><br><span class="line"><span class="string">   <span class="variable">$(lsb_release -cs)</span> \</span></span><br><span class="line"><span class="string">   stable"</span></span><br><span class="line">sudo apt-get update --fix-missing</span><br><span class="line">sudo apt-get install -y docker-ce docker-ce-cli containerd.io</span><br><span class="line">cat &lt;&lt;EOF | sudo tee /etc/docker/daemon.json</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"exec-opts"</span>: [<span class="string">"native.cgroupdriver=systemd"</span>],</span><br><span class="line">  <span class="string">"log-driver"</span>: <span class="string">"json-file"</span>,</span><br><span class="line">  <span class="string">"log-opts"</span>: &#123;</span><br><span class="line">    <span class="string">"max-size"</span>: <span class="string">"100m"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"storage-driver"</span>: <span class="string">"overlay2"</span></span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line">sudo mkdir -p /etc/systemd/system/docker.service.d</span><br><span class="line">sudo systemctl daemon-reload</span><br><span class="line">sudo systemctl restart docker</span><br><span class="line">sudo systemctl <span class="built_in">enable</span> docker</span><br></pre></td></tr></table></figure><h2 id="下载-Kubernetes-相关程序"><a href="#下载-Kubernetes-相关程序" class="headerlink" title="下载 Kubernetes 相关程序"></a>下载 Kubernetes 相关程序</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update &amp;&amp; sudo apt-get install -y apt-transport-https curl</span><br><span class="line">curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -</span><br><span class="line">cat &lt;&lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list</span><br><span class="line">deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main</span><br><span class="line">EOF</span><br><span class="line">sudo apt-get update --fix-missing</span><br><span class="line">sudo apt-get install -y kubelet kubeadm kubectl</span><br><span class="line">sudo apt-mark hold kubelet kubeadm kubectl</span><br></pre></td></tr></table></figure><h2 id="配置桥接网络防火墙"><a href="#配置桥接网络防火墙" class="headerlink" title="配置桥接网络防火墙"></a>配置桥接网络防火墙</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">EOF</span><br><span class="line">sudo sysctl --system</span><br></pre></td></tr></table></figure><h2 id="关闭-swap"><a href="#关闭-swap" class="headerlink" title="关闭 swap"></a>关闭 swap</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 临时关闭</span></span><br><span class="line">swapoff -a</span><br><span class="line"></span><br><span class="line"><span class="comment"># 永久关闭，注释 swap 配置</span></span><br><span class="line">vi /etc/fstab</span><br></pre></td></tr></table></figure><h2 id="关闭防火墙"><a href="#关闭防火墙" class="headerlink" title="关闭防火墙"></a>关闭防火墙</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl stop firewalld</span><br><span class="line">sudo systemctl <span class="built_in">disable</span> firewalld</span><br></pre></td></tr></table></figure><h2 id="关闭-selinux"><a href="#关闭-selinux" class="headerlink" title="关闭 selinux"></a>关闭 selinux</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo sed -i <span class="string">'s/enforcing/disabled/'</span> /etc/selinux/config</span><br><span class="line">sudo setenforce 0</span><br></pre></td></tr></table></figure><h2 id="拉取-gcr-镜像"><a href="#拉取-gcr-镜像" class="headerlink" title="拉取 gcr 镜像"></a>拉取 gcr 镜像</h2><p>需要在所有的 <strong>Master</strong> 节点与 <strong>Worker</strong> 节点拉取镜像。</p><p><strong>bash</strong> 环境使用如下脚本:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">image_list=$(kubeadm config images list)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> image <span class="keyword">in</span> <span class="variable">$&#123;image_list&#125;</span> ; <span class="keyword">do</span></span><br><span class="line">  name=$(<span class="built_in">echo</span> <span class="variable">$&#123;image&#125;</span> | cut -d<span class="string">'/'</span> -f2)</span><br><span class="line">  docker pull registry.aliyuncs.com/google_containers/<span class="variable">$name</span></span><br><span class="line">  docker image tag registry.aliyuncs.com/google_containers/<span class="variable">$name</span> k8s.gcr.io/<span class="variable">$name</span></span><br><span class="line">  docker image rm registry.aliyuncs.com/google_containers/<span class="variable">$name</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><p><strong>zsh</strong> 环境使用如下脚本:</p><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">image_list=$(kubeadm config images list)</span><br><span class="line">images=(`<span class="built_in">echo</span> <span class="variable">$&#123;image_list&#125;</span> | tr <span class="string">'\n'</span> <span class="string">' '</span>`)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> image <span class="keyword">in</span> <span class="variable">$&#123;images&#125;</span> ; <span class="keyword">do</span></span><br><span class="line">  name=$(<span class="built_in">echo</span> <span class="variable">$&#123;image&#125;</span> | cut -d<span class="string">'/'</span> -f2)</span><br><span class="line">  docker pull registry.aliyuncs.com/google_containers/<span class="variable">$name</span></span><br><span class="line">  docker image tag registry.aliyuncs.com/google_containers/<span class="variable">$name</span> k8s.gcr.io/<span class="variable">$name</span></span><br><span class="line">  docker image rm registry.aliyuncs.com/google_containers/<span class="variable">$name</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><h2 id="初始化-Master-节点"><a href="#初始化-Master-节点" class="headerlink" title="初始化 Master 节点"></a>初始化 Master 节点</h2><p>在 <strong>Master</strong> 节点执行命令:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">sudo kubeadm init \</span><br><span class="line">  --apiserver-advertise-address $(hostname -i) \</span><br><span class="line">  --pod-network-cidr 10.244.0.0/16 --v=5</span><br><span class="line"></span><br><span class="line">Your Kubernetes control-plane has initialized successfully!</span><br><span class="line"></span><br><span class="line">To start using your cluster, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">  mkdir -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">  sudo cp -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">  sudo chown $(id -u):$(id -g) <span class="variable">$HOME</span>/.kube/config</span><br><span class="line"></span><br><span class="line">You should now deploy a pod network to the cluster.</span><br><span class="line">Run <span class="string">"kubectl apply -f [podnetwork].yaml"</span> with one of the options listed at:</span><br><span class="line">  https://kubernetes.io/docs/concepts/cluster-administration/addons/</span><br><span class="line"></span><br><span class="line">Then you can join any number of worker nodes by running the following on each as root:</span><br><span class="line"></span><br><span class="line">kubeadm join 192.168.50.5:6443 --token 7zjyq9.x3xkoatt6pb1cbsu \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:1c78c44bc57e6e887c5f81e7a9c6c3e52f098e1ba9255f5303ac78129d410774</span><br></pre></td></tr></table></figure><p>或者省略下载镜像步骤，直接创建集群:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo kubeadm init \</span><br><span class="line">  --apiserver-advertise-address=$(hostname -i) \</span><br><span class="line">  --image-repository registry.aliyuncs.com/google_containers \</span><br><span class="line">  --service-cidr=10.5.0.0/16 \</span><br><span class="line">  --pod-network-cidr=10.244.0.0/16</span><br></pre></td></tr></table></figure><p>然后配置 <strong>kubeconfig</strong>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">sudo cp -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">sudo chown $(id -u):$(id -g) <span class="variable">$HOME</span>/.kube/config</span><br></pre></td></tr></table></figure><h2 id="配置网络"><a href="#配置网络" class="headerlink" title="配置网络"></a>配置网络</h2><h3 id="Calico"><a href="#Calico" class="headerlink" title="Calico"></a>Calico</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://docs.projectcalico.org/manifests/canal.yaml</span><br></pre></td></tr></table></figure><h2 id="添加-Worker-节点"><a href="#添加-Worker-节点" class="headerlink" title="添加 Worker 节点"></a>添加 Worker 节点</h2><p>在 <strong>Worker</strong> 节点执行命令:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo kubeadm join 192.168.50.5:6443 --token 4n2pwp.hq9jyo3auaibma3q     --discovery-token-ca-cert-hash sha256:750da2c87a67b96bfec73ade40888d22b61e045fdd28bbb7a4ff2c6ce3e0309c</span><br><span class="line"></span><br><span class="line">This node has joined the cluster:</span><br><span class="line">* Certificate signing request was sent to apiserver and a response was received.</span><br><span class="line">* The Kubelet was informed of the new secure connection details.</span><br><span class="line"></span><br><span class="line">Run <span class="string">'kubectl get nodes'</span> on the control-plane to see this node join the cluster.</span><br></pre></td></tr></table></figure><h2 id="测试集群"><a href="#测试集群" class="headerlink" title="测试集群"></a>测试集群</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://raw.githubusercontent.com/kubernetes/website/master/content/en/examples/application/nginx-app.yaml</span><br></pre></td></tr></table></figure><h2 id="清理集群"><a href="#清理集群" class="headerlink" title="清理集群"></a>清理集群</h2><p>在期望清理的节点执行:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo kubeadm reset</span><br><span class="line">sudo rm -rf /etc/cni/net.d</span><br><span class="line">rm -rf ~/.kube</span><br></pre></td></tr></table></figure><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="https://docs.docker.com/engine/install/ubuntu/" target="_blank" rel="noopener">Install Docker Engine on Ubuntu</a></li><li><a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/" target="_blank" rel="noopener">Container runtimes</a></li><li><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/" target="_blank" rel="noopener">Installing kubeadm</a></li><li><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/" target="_blank" rel="noopener">Creating a cluster with kubeadm</a></li><li><a href="https://github.com/cloudnativelabs/kube-router/blob/master/docs/kubeadm.md" target="_blank" rel="noopener">Deploying kube-router with kubeadm</a></li><li><a href="https://docs.projectcalico.org/getting-started/kubernetes/flannel/flannel" target="_blank" rel="noopener">Install Calico for policy and flannel (aka Canal) for networking</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在 &lt;strong&gt;Ubuntu 20.04&lt;/strong&gt; 系统上搭建 &lt;strong&gt;Kubernetes&lt;/strong&gt; 集群。&lt;/p&gt;
&lt;h2 id=&quot;安装-Docker&quot;&gt;&lt;a href=&quot;#安装-Docker&quot; class=&quot;headerlink&quot; titl</summary>
      
    
    
    
    
    <category term="kubernetes" scheme="http://divinerapier.github.io/tags/kubernetes/"/>
    
    <category term="kubeadm" scheme="http://divinerapier.github.io/tags/kubeadm/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu 20.04 配置静态网络</title>
    <link href="http://divinerapier.github.io/2020/10/19/configure-networks-on-ubuntu-20-04/"/>
    <id>http://divinerapier.github.io/2020/10/19/configure-networks-on-ubuntu-20-04/</id>
    <published>2020-10-19T10:55:44.000Z</published>
    <updated>2020-10-19T11:56:03.548Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Ubuntu 20.04</strong> 的网络配置文件位于 <strong>/etc/netplan/00-installer-config.yaml</strong>:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This is the network config written by 'subiquity'</span></span><br><span class="line"><span class="attr">network:</span></span><br><span class="line">  <span class="attr">ethernets:</span></span><br><span class="line">    <span class="attr">ens33:</span></span><br><span class="line">      <span class="attr">dhcp4:</span> <span class="literal">false</span></span><br><span class="line">      <span class="attr">addresses:</span> <span class="string">[192.168.50.30/24]</span></span><br><span class="line">      <span class="attr">gateway4:</span> <span class="number">192.168</span><span class="number">.50</span><span class="number">.1</span></span><br><span class="line">      <span class="attr">nameservers:</span></span><br><span class="line">        <span class="attr">addresses:</span> <span class="string">[192.168.50.1,</span> <span class="number">8.8</span><span class="number">.8</span><span class="number">.8</span><span class="string">]</span></span><br></pre></td></tr></table></figure><p>更新配置:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">root@ubuntu-00:~<span class="comment"># netplan --debug apply</span></span><br><span class="line">** (generate:3768): DEBUG: 11:49:11.734: Processing input file /etc/netplan/00-installer-config.yaml..</span><br><span class="line">** (generate:3768): DEBUG: 11:49:11.734: starting new processing pass</span><br><span class="line">** (generate:3768): DEBUG: 11:49:11.735: We have some netdefs, pass them through a final round of validation</span><br><span class="line">** (generate:3768): DEBUG: 11:49:11.735: ens33: setting default backend to 1</span><br><span class="line">** (generate:3768): DEBUG: 11:49:11.735: Configuration is valid</span><br><span class="line">** (generate:3768): DEBUG: 11:49:11.736: Generating output files..</span><br><span class="line">** (generate:3768): DEBUG: 11:49:11.736: NetworkManager: definition ens33 is not <span class="keyword">for</span> us (backend 1)</span><br><span class="line">(generate:3768): GLib-DEBUG: 11:49:11.736: posix_spawn avoided (fd close requested)</span><br><span class="line">DEBUG:netplan generated networkd configuration changed, restarting networkd</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;Ubuntu 20.04&lt;/strong&gt; 的网络配置文件位于 &lt;strong&gt;/etc/netplan/00-installer-config.yaml&lt;/strong&gt;:&lt;/p&gt;
&lt;figure class=&quot;highlight yaml&quot;&gt;&lt;table</summary>
      
    
    
    
    
    <category term="ubuntu" scheme="http://divinerapier.github.io/tags/ubuntu/"/>
    
    <category term="networks" scheme="http://divinerapier.github.io/tags/networks/"/>
    
  </entry>
  
  <entry>
    <title>Docker 中无法找到命令 configure</title>
    <link href="http://divinerapier.github.io/2020/10/18/command-configure-not-found-in-docker/"/>
    <id>http://divinerapier.github.io/2020/10/18/command-configure-not-found-in-docker/</id>
    <published>2020-10-18T13:02:27.000Z</published>
    <updated>2020-10-18T13:25:56.340Z</updated>
    
    <content type="html"><![CDATA[<p>通过进入基于 <code>ubuntu:20.04</code> 镜像运行的容器中安装 <strong>openmpi</strong> 的一系列指令得到了如下 <strong>dockerfile</strong> 片段:</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">RUN</span><span class="bash"> wget https://download.open-mpi.org/release/open-mpi/v4.0/openmpi-4.0.5.tar.gz</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> tar xzf openmpi-4.0.5.tar.gz -C /tmp</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> <span class="built_in">cd</span> /tmp/openmpi-4.0.5</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> ./configure --with-threads=posix --<span class="built_in">enable</span>-mpi-thread-multiple</span></span><br></pre></td></tr></table></figure><p>以上命令可以在容器中逐条执行，但却在构建镜像时失败:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/bin/sh: 1: ./configure: not found</span><br></pre></td></tr></table></figure><p>提示 <strong>configure</strong> 不存在。</p><p>修复办法是将 <strong>configure</strong> 命令与前一条命令合并:</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">RUN</span><span class="bash"> wget https://download.open-mpi.org/release/open-mpi/v4.0/openmpi-4.0.5.tar.gz</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> tar xzf openmpi-4.0.5.tar.gz -C /tmp</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> <span class="built_in">cd</span> /tmp/openmpi-4.0.5 \</span></span><br><span class="line"><span class="bash">  &amp;&amp; ./configure --with-threads=posix --<span class="built_in">enable</span>-mpi-thread-multiple \</span></span><br><span class="line"><span class="bash">  &amp;&amp; make -j \</span></span><br><span class="line"><span class="bash">  &amp;&amp; make -j install</span></span><br></pre></td></tr></table></figure><p>推测原因: <strong>docker</strong> 镜像的每一层基于不同的 <strong>workdir</strong>。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;通过进入基于 &lt;code&gt;ubuntu:20.04&lt;/code&gt; 镜像运行的容器中安装 &lt;strong&gt;openmpi&lt;/strong&gt; 的一系列指令得到了如下 &lt;strong&gt;dockerfile&lt;/strong&gt; 片段:&lt;/p&gt;
&lt;figure class=&quot;highl</summary>
      
    
    
    
    
    <category term="docker" scheme="http://divinerapier.github.io/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>GPU 101 - Architecture</title>
    <link href="http://divinerapier.github.io/2020/10/15/GPU-101-Architecture/"/>
    <id>http://divinerapier.github.io/2020/10/15/GPU-101-Architecture/</id>
    <published>2020-10-15T11:50:58.000Z</published>
    <updated>2020-10-15T12:42:54.195Z</updated>
    
    <content type="html"><![CDATA[<p><strong>图形处理器单元(GPU)</strong> 主要是指运行高度图形化应用时使用的硬件设备，比如 <strong>3D建模软件</strong> 或 <strong>VDI基础设施</strong>。在消费市场上，GPU多用于加速游戏图形。如今，GPGPU(General Purpose GPU) 是现代高性能计算 (HPC) 场景中加速计算的普遍硬件选择。</p><p>会用到 GPU 的领域除了我们熟悉的 HPC，比如会用到图像识别的机器学习方向，也常用在医疗、保险和金融行业相关的垂直领域中使用的 <a href="https://www.w3.org/TR/tabular-data-model/" target="_blank" rel="noopener">表格数据 (Tabular Data)</a> 的计算。</p><p>因此，就引出一个问题: 为什么需要使用 GPU 而不是 CPU?</p><h2 id="延迟与吞吐量"><a href="#延迟与吞吐量" class="headerlink" title="延迟与吞吐量"></a>延迟与吞吐量</h2><p>首先，来看看 CPU 和 GPU 的主要区别。</p><p>CPU 对速度与延迟方面进行了优化，在保持操作之间快速切换的能力的前提下，以尽可能低的延迟完成任务。它的本质就是以序列化的方式处理任务。</p><p>GPU 对吞吐量方面进行了优化，它允许一次推尽可能多的任务到内部，并通过并行方式处理每个任务。</p><p>下面的示例图显示了 CPU 和 GPU <strong>核心</strong> 的数量。显而易见地，GPU 具有更多的核心来处理一个任务。</p><p><img src="/images/GPU-101-Architecture/01-cpu-vs-gpu-cores.png" alt="cpu-vs-gpu-cores"></p><h2 id="异同点"><a href="#异同点" class="headerlink" title="异同点"></a>异同点</h2><p>然而，这不仅仅是核心数量的问题。而当我们说到NVIDIA GPU中的核心时，我们指的是由ALU（算术逻辑单元）组成的CUDA核心。不同厂商的术语可能会有所不同。</p><p>从CPU和GPU的整体架构来看，我们可以看到两者之间有很多相似之处。两者都使用了缓存层、内存控制器和全局内存的内存构造。对现代CPU架构的高层概述表明，它都是通过使用重要的缓存内存层来实现低延迟的内存访问。让我们先看一张图，它显示了一个通用的、以内存为中心的现代CPU封装（注意：精确的布局强烈地取决于供应商/型号）。</p><p><img src="/images/GPU-101-Architecture/02-cpu-hl-architecture.png" alt="cpu-hl-architecture"></p><p>一个CPU包由核心组成，包含独立的数据层和指令层-1缓存，由层-2缓存支持。第3层高速缓存，也就是最后一层高速缓存，由多个核心共享。如果数据没有驻留在缓存层中，它将从全局DDR-4内存中获取数据。每个CPU的核心数量可以达到28个或32个，根据品牌和型号的不同，在Turbo模式下可以运行到2.5GHz或3.8GHz。缓存大小范围为每个核心最高2MB二级缓存。</p><h2 id="探索GPU架构"><a href="#探索GPU架构" class="headerlink" title="探索GPU架构"></a>探索GPU架构</h2><p>如果我们检查GPU的高层架构概述（同样，强烈依赖make/model），看起来GPU的本质就是把可用的核心投入工作，它不太注重低延迟的缓存内存访问。</p><p><img src="/images/GPU-101-Architecture/03-gpu-architecture.png" alt="gpu-architecture"></p><p>单个GPU设备由多个处理器集群（PC）组成，其中包含多个流式多处理器（SM）。每个SM容纳一个第1层指令缓存层与其相关的核心。通常情况下，一个SM在从全局GDDR-5内存中提取数据之前，会使用一个专用的第1层缓存和一个共享的第2层缓存。其架构对内存延迟的容忍度很高。</p><p>与CPU相比，GPU工作的内存缓存层数较少，而且相对较小。原因是GPU有更多的晶体管用于计算，这意味着它不太在乎从内存中检索数据所需的时间。只要GPU手头有足够的计算量，潜在的内存访问 “延迟 “就会被掩盖，使其保持忙碌。</p><blockquote><p>A GPU is optimized for data parallel throughput computations.</p></blockquote><p>从核心数量来看，它很快就能让你看到它在并行方面的可能性。 当检查当前NVIDIA的旗舰产品Tesla V100时，一台设备包含80个SM，每个SM包含64个核心，总共有5120个核心！任务不是安排给单个核心，而是安排给处理器集群和SM。任务不是安排给单个核心，而是安排给处理器集群和SM。这就是它能够并行处理的原因。现在把这个强大的硬件设备和编程框架结合起来，应用就可以充分发挥GPU的计算能力。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://nielshagoort.com/2019/03/12/exploring-the-gpu-architecture/" target="_blank" rel="noopener">Exploring the GPU Architecture</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;图形处理器单元(GPU)&lt;/strong&gt; 主要是指运行高度图形化应用时使用的硬件设备，比如 &lt;strong&gt;3D建模软件&lt;/strong&gt; 或 &lt;strong&gt;VDI基础设施&lt;/strong&gt;。在消费市场上，GPU多用于加速游戏图形。如今，GPGPU(Ge</summary>
      
    
    
    
    
    <category term="gpu" scheme="http://divinerapier.github.io/tags/gpu/"/>
    
  </entry>
  
  <entry>
    <title>WSL2 中无法连接 Docker 服务</title>
    <link href="http://divinerapier.github.io/2020/10/15/cannot-connect-to-docker-daemon-on-wsl2/"/>
    <id>http://divinerapier.github.io/2020/10/15/cannot-connect-to-docker-daemon-on-wsl2/</id>
    <published>2020-10-15T09:21:20.000Z</published>
    <updated>2020-10-15T09:31:37.955Z</updated>
    
    <content type="html"><![CDATA[<p>在 <strong>WSL2</strong> 环境中使用 <strong>Docker</strong> 遇到错误:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker ps</span><br><span class="line">Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?</span><br></pre></td></tr></table></figure><p>可以通过如下操作解决:</p><ol><li><p>在 <strong>App and features</strong> 中卸载 <strong>Docker</strong> 程序</p></li><li><p>删除如下目录</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">C:\Program Files\Docker</span><br><span class="line">C:\ProgramData\DockerDesktop</span><br><span class="line">C:\Users\[USERNAME]\.docker</span><br><span class="line">C:\Users\[USERNAME]\AppData\Local\Docker</span><br><span class="line">C:\Users\[USERNAME]\AppData\Roaming\Docker</span><br><span class="line">C:\Users\[USERNAME]\AppData\Roaming\Docker Desktop</span><br></pre></td></tr></table></figure></li><li><p>下载最 <a href="https://docs.docker.com/docker-for-windows/edge-release-notes/" target="_blank" rel="noopener">新版本 <strong>Docker</strong></a></p></li><li><p>启动 <strong>Docker</strong></p></li></ol><p>到此为止，问题应该已经被解决了(至少我解决了)。</p><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="https://codesthq.com/painless-way-to-wsl-2-with-docker/" target="_blank" rel="noopener">Painless way to WSL 2 with Docker</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在 &lt;strong&gt;WSL2&lt;/strong&gt; 环境中使用 &lt;strong&gt;Docker&lt;/strong&gt; 遇到错误:&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span c</summary>
      
    
    
    
    
    <category term="windows" scheme="http://divinerapier.github.io/tags/windows/"/>
    
    <category term="wsl2" scheme="http://divinerapier.github.io/tags/wsl2/"/>
    
    <category term="docker" scheme="http://divinerapier.github.io/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>初识 MPI</title>
    <link href="http://divinerapier.github.io/2020/10/11/started-with-mpi/"/>
    <id>http://divinerapier.github.io/2020/10/11/started-with-mpi/</id>
    <published>2020-10-11T04:10:37.000Z</published>
    <updated>2020-10-11T13:08:47.240Z</updated>
    
    <content type="html"><![CDATA[<h2 id="消息传递模型"><a href="#消息传递模型" class="headerlink" title="消息传递模型"></a>消息传递模型</h2><p><strong>消息传递模型(Message Passing Model)</strong> 指程序通过在进程间传递消息（消息可以理解成带有一些信息和数据的一个数据结构）来完成某些任务。在实践中，基于此模型，很容易开发 <strong>并发程序</strong>。</p><p>举例来说:</p><ol><li>主进程(manager process) 可以通过向从进程(worker process) 发送一个描述工作的消息的方式，将工作分配给从进程。</li><li>一个并发的排序程序可以在当前进程中对当前进程可见的(我们称作本地的，locally) 数据进行排序，然后把排好序的数据发送到邻居进程上面来进行合并的操作。</li></ol><p>几乎所有的并行程序可以使用消息传递模型来描述。</p><p>之后，业界统一制定了一套消息传递模型的接口标准，即 <strong>Message Passing Interface —— MPI</strong>。</p><h2 id="MPI-基础概念"><a href="#MPI-基础概念" class="headerlink" title="MPI 基础概念"></a>MPI 基础概念</h2><h3 id="Communicator"><a href="#Communicator" class="headerlink" title="Communicator"></a>Communicator</h3><p><strong>通讯器(communicator)</strong> 定义了一组能够互相发消息的进程。</p><h3 id="Rank"><a href="#Rank" class="headerlink" title="Rank"></a>Rank</h3><p>在 <strong>通讯器(communicator)</strong> 中，每个进程会被分配一个序号，称作 <strong>秩(rank)</strong>，进程间显性地通过指定 <strong>rank</strong> 来进行通信。</p><h3 id="Tag"><a href="#Tag" class="headerlink" title="Tag"></a>Tag</h3><p>不同进程之间发送、接收操作是通信的基础。</p><p>作为发送者时，进程可以通过指定另一个进程的 <strong>rank</strong> 和一个独一无二的 <strong>消息标签(tag)</strong> 来发送消息给另一个进程。</p><p>作为接受者时，进程可以发送一个 <strong>接收特定标签标记的消息的请求 (或者忽略标签，接收任何消息)</strong>，然后依次处理接收到的数据。</p><h3 id="Point-to-Point-Communications"><a href="#Point-to-Point-Communications" class="headerlink" title="Point-to-Point Communications"></a>Point-to-Point Communications</h3><p>一个发送者，一个接受者的通信被称作 <strong>点对点(point-to-point) 通信</strong>。</p><h3 id="Collective-Communications"><a href="#Collective-Communications" class="headerlink" title="Collective Communications"></a>Collective Communications</h3><p>在很多情况下，某个进程可能需要跟所有其他进程通信。比如主进程想发一个广播给所有的从进程。在这种情况下，如果通过写代码的方式来完成所有的发送和接收过程会很麻烦。并且，事实上，这种方式往往也不会以最佳方式使用网络。MPI 可以处理各种各样的这些涉及所有进程的 <strong>集体(Collective)通信</strong> 类型。</p><h2 id="使用-MPI"><a href="#使用-MPI" class="headerlink" title="使用 MPI"></a>使用 MPI</h2><p><strong>MPI</strong> 只是一套接口标准，无法直接使用。对此不必担心，业内已经存在很多符合标准的实现。其中 <strong>OpenMPI</strong> 就是最受欢迎的实现之一。因此，之后的内容基于 <strong>OpenMPI</strong> 展开。</p><h3 id="安装-OpenMPI"><a href="#安装-OpenMPI" class="headerlink" title="安装 OpenMPI"></a>安装 OpenMPI</h3><p>从 <a href="https://www.open-mpi.org/software/ompi/v4.0/" target="_blank" rel="noopener">这里</a> 可以找到最新的版本，本文基于版本 <a href="https://download.open-mpi.org/release/open-mpi/v4.0/openmpi-4.0.5.tar.gz" target="_blank" rel="noopener">4.0.5</a>。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># download package</span></span><br><span class="line">$ wget https://download.open-mpi.org/release/open-mpi/v4.0/openmpi-4.0.5.tar.gz</span><br><span class="line"></span><br><span class="line"><span class="comment"># extract files</span></span><br><span class="line">$ tar xzf openmpi-4.0.5.tar.gz</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cd</span> openmpi-4.0.5</span><br><span class="line"></span><br><span class="line"><span class="comment"># configure project</span></span><br><span class="line">$ mkdir -p build; ./configure --prefix=$(<span class="built_in">pwd</span>)/build</span><br><span class="line"></span><br><span class="line"><span class="comment"># build</span></span><br><span class="line">$ make -j; make -j install</span><br></pre></td></tr></table></figure><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="https://mpitutorial.com/tutorials/mpi-introduction/" target="_blank" rel="noopener">MPI Tutorial Introduction</a></li><li><a href="https://www.open-mpi.org/faq/" target="_blank" rel="noopener">OpenMPI FAQ</a></li><li><a href="https://www.mpi-forum.org/" target="_blank" rel="noopener">MPI Forum</a></li><li><a href="https://www.citutor.org//browse.php" target="_blank" rel="noopener">The “Introduction to MPI” and “Intermediate MPI” tutorials</a></li><li><a href="http://hpc.mediawiki.hull.ac.uk/Applications/OpenMPI" target="_blank" rel="noopener">UNIVERSITY OF HULL HPC: OpenMPI</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;消息传递模型&quot;&gt;&lt;a href=&quot;#消息传递模型&quot; class=&quot;headerlink&quot; title=&quot;消息传递模型&quot;&gt;&lt;/a&gt;消息传递模型&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;消息传递模型(Message Passing Model)&lt;/strong&gt; 指程序通过在进</summary>
      
    
    
    
    
    <category term="mpi" scheme="http://divinerapier.github.io/tags/mpi/"/>
    
  </entry>
  
  <entry>
    <title>训练系统相关读物</title>
    <link href="http://divinerapier.github.io/2020/10/11/readings-about-training-system/"/>
    <id>http://divinerapier.github.io/2020/10/11/readings-about-training-system/</id>
    <published>2020-10-11T03:44:32.000Z</published>
    <updated>2020-10-11T06:44:19.060Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="https://images.nvidia.com/events/sc15/pdfs/NCCL-Woolley.pdf" target="_blank" rel="noopener">NCCL: ACCELERATED MULTI-GPU COLLECTIVE COMMUNICATIONS</a></li><li><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.95.2490&rep=rep1&type=pdf" target="_blank" rel="noopener">Message Passing, Remote Procedure Calls and Distributed Shared Memory as Communication Paradigms for Distributed Systems</a></li><li><a href="https://zhuanlan.zhihu.com/p/50116885" target="_blank" rel="noopener">分布式训练的方案和效率对比</a></li><li><a href="http://gaocegege.com/Blog/mpi-1" target="_blank" rel="noopener">MPI，OpenMPI 与深度学习</a></li><li><a href="https://mpitutorial.com/tutorials/mpi-introduction/zh_cn/" target="_blank" rel="noopener">MPI 教程介绍</a></li><li><a href="https://www.mpi-forum.org/" target="_blank" rel="noopener">MPI Forum</a></li><li><a href="https://zhuanlan.zhihu.com/p/149771261" target="_blank" rel="noopener">2020 Rethinking GPU 集群上的分布式训练</a></li><li><a href="https://zhuanlan.zhihu.com/p/40578792" target="_blank" rel="noopener">Horovod-基于TensorFlow分布式深度学习框架</a></li><li><a href="https://github.com/horovod/horovod" target="_blank" rel="noopener">Github: Horovod</a></li><li><a href="https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/" target="_blank" rel="noopener">Schedule GPUs</a></li><li><a href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/" target="_blank" rel="noopener">Device Plugins</a></li><li><a href="https://towardsdatascience.com/distributed-deep-learning-training-with-horovod-on-kubernetes-6b28ac1d6b5d" target="_blank" rel="noopener">Distributed Deep Learning Training with Horovod on Kubernetes</a></li><li><a href="https://developer.nvidia.com/kubernetes-gpu" target="_blank" rel="noopener">Kubernetes on NVIDIA GPUs</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://images.nvidia.com/events/sc15/pdfs/NCCL-Woolley.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;NCCL: ACCELERATED MULTI-GPU CO</summary>
      
    
    
    
    
    <category term="gpu" scheme="http://divinerapier.github.io/tags/gpu/"/>
    
    <category term="kubernetes" scheme="http://divinerapier.github.io/tags/kubernetes/"/>
    
    <category term="mpi" scheme="http://divinerapier.github.io/tags/mpi/"/>
    
    <category term="rdma" scheme="http://divinerapier.github.io/tags/rdma/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 存储卷</title>
    <link href="http://divinerapier.github.io/2020/10/07/kubernetes-volumes/"/>
    <id>http://divinerapier.github.io/2020/10/07/kubernetes-volumes/</id>
    <published>2020-10-07T09:37:14.000Z</published>
    <updated>2020-10-12T05:54:15.720Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在 <strong>Docker</strong> 中，<strong><a href="https://docs.docker.com/storage/" target="_blank" rel="noopener">Volume</a></strong> 概念表示磁盘上或者另外一个容器内的一个目录。 直到最近，Docker 才支持对基于本地磁盘的 Volume 的生存期进行管理。 虽然 Docker 现在也能提供 Volume 驱动程序，但是目前功能还非常有限 （例如，截至 Docker 1.7，每个容器只允许有一个 Volume 驱动程序，并且无法将参数传递给 Volume）。</p><p>而在 <strong>Kubernetes</strong> 中，<strong>Volume</strong> 具有明确的生命周期——与其所属 <strong>Pod</strong> 相同。 因此，<strong>Volume 比 Pod 中运行的任何容器的存活期都长</strong>，在容器重新启动时数据也会得到保留。 当然，<strong>当一个 Pod 不再存在时，卷也将不再存在</strong>。 更重要的是，<strong>Kubernetes 可以支持许多类型的卷，Pod 也能同时使用任意数量的卷</strong>。</p><p><strong>Volume</strong> 的核心是包含一些数据的目录，Pod 中的容器可以访问该目录。 特定的卷类型可以决定这个目录是如何形成的，并能决定它支持何种介质，以及目录中存放什么内容。</p><p>使用 <strong>Volume</strong> 时, Pod 声明中需要提供卷的类型 (<strong>.spec.volumes</strong> 字段) 和 <strong>Volume</strong> 挂载的位置 (<strong>.spec.containers.volumeMounts</strong> 字段).</p><p>容器中的进程能看到由它们的 Docker 镜像和卷组成的文件系统视图。 <a href="https://docs.docker.com/userguide/dockerimages/" target="_blank" rel="noopener">Docker 镜像</a> 位于文件系统层次结构的根部，并且任何 Volume 都挂载在镜像内的指定路径上。 卷不能挂载到其他卷，也不能与其他卷有硬链接。 Pod 中的每个容器必须独立地指定每个卷的挂载位置(<strong>Volumes</strong> 之间的挂载点应该相互独立)。</p><h2 id="Volume-的类型"><a href="#Volume-的类型" class="headerlink" title="Volume 的类型"></a>Volume 的类型</h2><p>Kubernetes 支持下列类型的卷:</p><ul><li><a href="#Cephfs">cephfs</a></li><li><a href="#ConfigMap">configMap</a></li><li><a href="#CSI">csi</a></li><li><a href="#DownwardAPI">downwardAPI</a></li><li><a href="#EmptyDir">emptyDir</a></li><li><a href="#HostPath">hostPath</a></li><li><a href="#Local">local</a></li><li><a href="#Nfs">nfs</a></li><li><a href="#PersistentVolumeClaim">persistentVolumeClaim</a></li><li><a href="#Projected">projected</a></li><li><a href="#Secret">secret</a></li></ul><h3 id="Cephfs"><a href="#Cephfs" class="headerlink" title="Cephfs"></a>Cephfs</h3><p><strong>cephfs</strong> 允许用户将现存的 <strong>CephFS</strong> 卷挂载到 <strong>Pod</strong> 中。 与 <strong><a href="#EmptyDir">emptyDir</a></strong> 不同的是，<strong>emptyDir</strong> 会在删除 Pod 的同时<strong>一并被删除</strong>，<strong>cephfs</strong> 卷的内容在删除 Pod 时会被保留，卷只是被卸载掉了。 这意味着 <strong>CephFS 卷可以被预先填充数据，并且这些数据可以在 Pod 之间”传递”</strong>。CephFS 卷可同时被多个写者挂载。</p><blockquote><p>注意： 在您使用 Ceph 卷之前，您的 Ceph 服务器必须正常运行并且要使用的 share 被导出（exported）。</p></blockquote><p>更多信息请参考 <a href="https://github.com/kubernetes/examples/tree/master/volumes/cephfs/" target="_blank" rel="noopener">CephFS 示例</a>。</p><h3 id="ConfigMap"><a href="#ConfigMap" class="headerlink" title="ConfigMap"></a>ConfigMap</h3><p><strong>configMap</strong> 资源提供了向 Pod <strong>注入配置数据</strong>的方法。 ConfigMap 对象中存储的数据可以被 configMap 类型的卷引用，然后被应用到 Pod 中运行的容器化应用。</p><p>当引用 configMap 对象时，你可以简单的在 Volume 中通过它名称来引用。 还可以自定义 ConfigMap 中特定条目所要使用的路径。 例如，要将名为 log-config 的 ConfigMap 挂载到名为 configmap-pod 的 Pod 中，您可以使用下面的 YAML:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">configmap-pod</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">test</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">      <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-vol</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/etc/config</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-vol</span></span><br><span class="line">      <span class="attr">configMap:</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">log-config</span></span><br><span class="line">        <span class="attr">items:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">log_level</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">log_level</span></span><br></pre></td></tr></table></figure><h3 id="CSI"><a href="#CSI" class="headerlink" title="CSI"></a>CSI</h3><p><a href="https://github.com/container-storage-interface/spec/blob/master/spec.md" target="_blank" rel="noopener">容器存储接口 (CSI)</a> 为容器编排系统（如 Kubernetes）定义标准接口，以将任意存储系统暴露给它们的容器工作负载。</p><p>更多详情请阅读 <a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md" target="_blank" rel="noopener">CSI 设计方案</a>。</p><p>CSI 的支持在 Kubernetes v1.9 中作为 alpha 特性引入，在 Kubernetes v1.10 中转为 beta 特性，并在 Kubernetes v1.13 正式 GA。</p><blockquote><p><strong>说明:</strong> CSI驱动程序可能并非在所有Kubernetes版本中都兼容。 请查看特定CSI驱动程序的文档，以获取每个 Kubernetes 版本所支持的部署步骤以及兼容性列表。</p></blockquote><p>一旦在 <strong>Kubernetes</strong> 集群上部署了 CSI 兼容卷驱动程序，用户就可以使用 <strong>csi</strong> 作为卷类型来关联、挂载 <strong>CSI Driver</strong> 暴露出来的卷。</p><p>允许如下三种方式，在 Pod 中使用 <strong>csi</strong> 类型的卷:</p><ul><li>通过 <strong>PersistentVolumeClaim</strong></li><li>通过 <strong><a href="https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/#generic-ephemeral-volumes" target="_blank" rel="noopener">Generic ephemeral volumes</a></strong></li><li>通过 <strong><a href="https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volumes" target="_blank" rel="noopener">CSI ephemeral volumes</a></strong></li></ul><p>存储管理员可以使用以下字段来配置 CSI 持久卷(CSI persistent volume):</p><ul><li><p><strong>driver</strong>：指定要使用的卷 驱动程序(CSI Driver) 名称的字符串值。 这个值必须与 <strong>CSI Driver</strong> 的 <strong><a href="(https://github.com/container-storage-interface/spec/blob/master/spec.md#getplugininfo)">GetPluginInfoResponse</a></strong> 的 <strong>name</strong> 字段相同。 <strong>Kubernetes</strong> 使用所给的值来标识要调用的 <strong>CSI Driver</strong>；<strong>CSI Driver</strong> 也使用该值来<strong>辨识哪些 PV 对象属于该 CSI Driver</strong>。</p></li><li><p><strong>volumeHandle</strong>：唯一标识卷的字符串值。 该值必须与 <strong>CSI Driver</strong> 的 <strong><a href="https://github.com/container-storage-interface/spec/blob/master/spec.md#createvolume" target="_blank" rel="noopener">CreateVolumeResponse</a></strong> 的 <strong>volume.id</strong> 字段相同。 在所有对 <strong>CSI Driver</strong> 的调用中，引用该 <strong>Volume</strong> 时都使用此值作为 <strong>volume_id</strong> 参数。</p></li><li><p><strong>readOnly</strong>：一个可选的布尔值，指示通过 <strong>ControllerPublished</strong> 关联该卷时是否设置该卷为只读。 <strong>默认值是 false</strong>。 该值通过 <strong><a href="https://github.com/container-storage-interface/spec/blob/master/spec.md#controllerpublishvolume" target="_blank" rel="noopener">ControllerPublishVolumeRequest</a></strong> 中的 <strong>readonly</strong> 字段传递给 <strong>CSI Driver</strong>。</p></li><li><p><strong>fsType</strong>：如果 <strong>PV</strong> 的 <strong>VolumeMode</strong> 为 <strong>Filesystem</strong>，则该字段指定挂载卷时应该使用的文件系统。 倘若 <strong>Volume</strong> 尚未完成格式化，且支持格式化，则该值将被用于格式化。 可以通过 <strong><a href="https://github.com/container-storage-interface/spec/blob/master/spec.md#controllerpublishvolume" target="_blank" rel="noopener">ControllerPublishVolumeRequest</a></strong>、<strong><a href="https://github.com/container-storage-interface/spec/blob/master/spec.md#nodestagevolume" target="_blank" rel="noopener">NodeStageVolumeRequest</a></strong> 和 <strong><a href="https://github.com/container-storage-interface/spec/blob/master/spec.md#nodepublishvolume" target="_blank" rel="noopener">NodePublishVolumeRequest</a></strong> 的 <strong>volume_capability</strong> 字段将该值传递给 <strong>CSI Driver</strong>。</p></li><li><p><strong>volumeAttributes</strong>：一个 <strong>map[string]string</strong> 类型的映射表，用来设置 <strong>Volume</strong> 的静态属性。 该映射表必须与 <strong>CSI Driver</strong> 返回的 <strong><a href="https://github.com/container-storage-interface/spec/blob/master/spec.md#createvolume" target="_blank" rel="noopener">CreateVolumeResponse</a></strong> 中的 volume.attributes 字段的映射相对应。 该映射表通过 <strong><a href="https://github.com/container-storage-interface/spec/blob/master/spec.md#controllerpublishvolume" target="_blank" rel="noopener">ControllerPublishVolumeRequest</a></strong>、<strong><a href="https://github.com/container-storage-interface/spec/blob/master/spec.md#nodestagevolume" target="_blank" rel="noopener">NodeStageVolumeRequest</a></strong>、和 <strong><a href="https://github.com/container-storage-interface/spec/blob/master/spec.md#nodepublishvolume" target="_blank" rel="noopener">NodePublishVolumeRequest</a></strong> 中的 <strong>volume_attributes</strong> 字段传递给 <strong>CSI Driver</strong>。</p><ul><li><strong>注意</strong>: 在 <a href="https://github.com/container-storage-interface/spec/blob/master/spec.md" target="_blank" rel="noopener">spec</a> 中只看到了 <strong>volume_context</strong>，并没有 <strong>attributes</strong>，根据注释与数据类型来分析，或许是指这个字段？</li></ul></li><li><p><strong>controllerPublishSecretRef</strong>：对包含敏感信息的 secret 对象的引用；该敏感信息会被传递给 <strong>CSI Driver</strong> 来完成 <strong><a href="https://github.com/container-storage-interface/spec/blob/master/spec.md#controllerpublishvolume" target="_blank" rel="noopener">ControllerPublishVolume</a></strong> 和 <strong><a href="https://github.com/container-storage-interface/spec/blob/master/spec.md#controllerunpublishvolume" target="_blank" rel="noopener">ControllerUnpublishVolume</a></strong> 调用。 该字段为可选字段；为空表示不需要 secret。 如果 secret 对象包含多个 secret，则所有的 secret 都会被传递。</p></li><li><p><strong>nodeStageSecretRef</strong>：对包含敏感信息的 secret 对象的引用，以传递给 <strong>CSI Driver</strong> 来完成 <strong><a href="https://github.com/container-storage-interface/spec/blob/master/spec.md#nodestagevolume" target="_blank" rel="noopener">NodeStageVolume</a></strong> 调用。 该字段为可选字段；为空表示不需要 secret。 如果 secret 对象包含多个 secret，则所有的 secret 都会被传递。</p></li><li><p><strong>nodePublishSecretRef</strong>：对包含敏感信息的 secret 对象的引用，以传递给 <strong>CSI Driver</strong> 来完成 <strong><a href="https://github.com/container-storage-interface/spec/blob/master/spec.md#nodepublishvolume" target="_blank" rel="noopener">NodePublishVolume</a></strong> 调用。 该字段为可选字段；为空表示不需要 secret。 如果 secret 对象包含多个 secret，则所有的 secret 都会被传递。</p></li></ul><h3 id="DownwardAPI"><a href="#DownwardAPI" class="headerlink" title="DownwardAPI"></a>DownwardAPI</h3><p><strong>downwardAPI</strong> 类型的 <strong>Volume</strong> 被用于使 <strong>downward API</strong> 数据对应用程序可见。其表现形式为，挂载一个目录，并将请求的数据写入到纯文本文件中。</p><h3 id="EmptyDir"><a href="#EmptyDir" class="headerlink" title="EmptyDir"></a>EmptyDir</h3><p>当 <strong>Pod</strong> 被指定到某个节点上时，首先创建的是一个 <strong>emptyDir</strong> 类型的 <strong>Volume</strong>，并且只要 <strong>Pod</strong> 保持在该节点上运行，<strong>Volume</strong> 就一直存在。正如名字所说的那样，<strong>Volume</strong> 的初始状态为空。虽然 <strong>Pod</strong> 中的容器挂载 <strong>emptyDir</strong> 类型 <strong>Volume</strong> 的路径可能不尽相同，但这都不重要，重要的是，这些容器都可以读写 <strong>emptyDir</strong> 类型 <strong>Volume</strong> 中的相同的文件。 无论因何种原因，只要 <strong>Pod</strong> 从节点上被删除，<strong>emptyDir</strong> 类型的 <strong>Volume</strong> 中的数据也会被永久删除。</p><blockquote><p><strong>说明</strong>: 容器崩溃并不会导致 <strong>Pod</strong> 从节点上被移除，因此容器崩溃时 <strong>emptyDir</strong> 类型 <strong>Volume</strong> 中的数据是安全的。</p></blockquote><p>有如下需求可以考虑使用 <strong>emptyDir</strong> 类型 <strong>Volume</strong>:</p><ul><li>缓存空间，例如基于磁盘的归并排序。</li><li>为耗时较长的计算任务提供检查点，以便任务能方便地从崩溃前状态恢复执行。</li><li>在 Web 服务器容器服务数据时，保存内容管理器容器获取的文件。</li></ul><p>默认情况下， <strong>emptyDir volume</strong> 所使用的的实际存储介质由节点使用何种存储介质决定: 可以是 <strong>HDD</strong> 或 <strong>SSD</strong> 或 <strong>NFS</strong> 等。但是，可以令 <strong>emptyDir.medium = Memory</strong> 使 <strong>Kubernetes</strong> 安装 <strong>tmpfs</strong>。但需要考虑到，<strong>tmpfs</strong> 的优势与劣势都很突出:</p><ul><li>优势: 基于 <strong>RAM</strong> 的文件系统，速度非常快</li><li>劣势: 随节点重启被清除，且写入的所有文件都会计入容器的内存消耗，受容器内存限制约束</li></ul><h4 id="EmptyDir-示例"><a href="#EmptyDir-示例" class="headerlink" title="EmptyDir 示例"></a>EmptyDir 示例</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">test-pd</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">k8s.gcr.io/test-webserver</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">test-container</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/cache</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">cache-volume</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">cache-volume</span></span><br><span class="line">    <span class="attr">emptyDir:</span> <span class="string">&#123;&#125;</span></span><br></pre></td></tr></table></figure><h3 id="HostPath"><a href="#HostPath" class="headerlink" title="HostPath"></a>HostPath</h3><p>A hostPath volume mounts a file or directory from the host node’s filesystem into your Pod. This is not something that most Pods will need, but it offers a powerful escape hatch for some applications.</p><p><strong>hostPath</strong> 类型的 <strong>Volume</strong> 会将宿主机节点的路径挂载到</p><p>For example, some uses for a hostPath are:</p><p>running a Container that needs access to Docker internals; use a hostPath of /var/lib/docker<br>running cAdvisor in a Container; use a hostPath of /sys<br>allowing a Pod to specify whether a given hostPath should exist prior to the Pod running, whether it should be created, and what it should exist as<br>In addition to the required path property, user can optionally specify a type for a hostPath volume.</p><p>The supported values for field type are:</p><p>Value    Behavior        Empty string (default) is for backward compatibility, which means that no checks will be performed before mounting the hostPath volume.    DirectoryOrCreate    If nothing exists at the given path, an empty directory will be created there as needed with permission set to 0755, having the same group and ownership with Kubelet.    Directory    A directory must exist at the given path    FileOrCreate    If nothing exists at the given path, an empty file will be created there as needed with permission set to 0644, having the same group and ownership with Kubelet.    File    A file must exist at the given path    Socket    A UNIX socket must exist at the given path    CharDevice    A character device must exist at the given path    BlockDevice    A block device must exist at the given path<br>Watch out when using this type of volume, because:</p><p>Pods with identical configuration (such as created from a podTemplate) may behave differently on different nodes due to different files on the nodes<br>when Kubernetes adds resource-aware scheduling, as is planned, it will not be able to account for resources used by a hostPath<br>the files or directories created on the underlying hosts are only writable by root. You either need to run your process as root in a privileged Container or modify the file permissions on the host to be able to write to a hostPath volume</p><h3 id="Local"><a href="#Local" class="headerlink" title="Local"></a>Local</h3><h3 id="Nfs"><a href="#Nfs" class="headerlink" title="Nfs"></a>Nfs</h3><h3 id="PersistentVolumeClaim"><a href="#PersistentVolumeClaim" class="headerlink" title="PersistentVolumeClaim"></a>PersistentVolumeClaim</h3><h3 id="Projected"><a href="#Projected" class="headerlink" title="Projected"></a>Projected</h3><h3 id="Secret"><a href="#Secret" class="headerlink" title="Secret"></a>Secret</h3><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/" target="_blank" rel="noopener">Volumes</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;在 &lt;strong&gt;Docker&lt;/strong&gt; 中，&lt;strong&gt;&lt;a href=&quot;https://docs.docker.com/s</summary>
      
    
    
    
    
    <category term="kubernetes" scheme="http://divinerapier.github.io/tags/kubernetes/"/>
    
    <category term="storage" scheme="http://divinerapier.github.io/tags/storage/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 持久卷</title>
    <link href="http://divinerapier.github.io/2020/10/07/kubernetes-persistent-volume/"/>
    <id>http://divinerapier.github.io/2020/10/07/kubernetes-persistent-volume/</id>
    <published>2020-10-07T09:30:27.000Z</published>
    <updated>2020-10-11T05:17:14.310Z</updated>
    
    
    
    
    
    <category term="kubernetes" scheme="http://divinerapier.github.io/tags/kubernetes/"/>
    
    <category term="volume" scheme="http://divinerapier.github.io/tags/volume/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 控制器</title>
    <link href="http://divinerapier.github.io/2020/10/06/kubernetes-controllers/"/>
    <id>http://divinerapier.github.io/2020/10/06/kubernetes-controllers/</id>
    <published>2020-10-06T05:13:35.000Z</published>
    <updated>2020-10-09T07:58:21.747Z</updated>
    
    <content type="html"><![CDATA[<p>在机器人技术和自动化领域，控制回路（Control Loop）是一个非终止回路，用于调节系统状态。</p><p>这是一个控制环的例子：房间里的温度自动调节器。</p><p>当你设置了温度，告诉了温度自动调节器你的期望状态（Desired State）。 房间的实际温度是当前状态（Current State）。 通过对设备的开关控制，温度自动调节器让其当前状态接近期望状态。</p><p>控制器通过 <a href="https://kubernetes.io/docs/reference/generated/kube-apiserver/" target="_blank" rel="noopener">apiserver</a> 监控集群的公共状态，并致力于将当前状态转变为期望的状态。</p><h2 id="控制器模式"><a href="#控制器模式" class="headerlink" title="控制器模式"></a>控制器模式</h2><p>一个控制器至少追踪一种类型的 Kubernetes 资源。这些 <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/" target="_blank" rel="noopener">对象</a> 有一个代表期望状态的 spec 字段。 该资源的控制器负责确保其当前状态接近期望状态。</p><p>控制器可能会自行执行操作；在 Kubernetes 中更常见的是一个控制器会发送信息给 <a href="https://kubernetes.io/docs/reference/generated/kube-apiserver/" target="_blank" rel="noopener">API 服务器</a>，这会有副作用。 具体可参看后文的例子。</p><h3 id="通过-API-服务器来控制"><a href="#通过-API-服务器来控制" class="headerlink" title="通过 API 服务器来控制"></a>通过 API 服务器来控制</h3><p><a href="https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion" target="_blank" rel="noopener">Job</a> 控制器是一个 Kubernetes 内置控制器的例子。 内置控制器通过和集群 API 服务器交互来管理状态。</p><p>Job 是一种 Kubernetes 资源，它运行一个或者多个 Pod， 来执行一个任务然后停止。 （一旦被调度了，对 kubelet 来说 Pod 对象就会变成了期望状态的一部分）。</p><p>在集群中，当 Job 控制器拿到新任务时，它会保证一组 Node 节点上的 kubelet 可以运行正确数量的 Pod 来完成工作。 Job 控制器不会自己运行任何的 Pod 或者容器。Job 控制器是通知 API 服务器来创建或者移除 Pod。<a href="https://kubernetes.io/docs/reference/glossary/?all=true#term-control-plane" target="_blank" rel="noopener">控制面</a>中的其它组件 根据新的消息作出反应（调度并运行新 Pod）并且最终完成工作。</p><p>创建新 Job 后，所期望的状态就是完成这个 Job。Job 控制器会让 Job 的当前状态不断接近期望状态：创建为 Job 要完成工作所需要的 Pod，使 Job 的状态接近完成。</p><p>控制器也会更新配置对象。例如：一旦 Job 的工作完成了，Job 控制器会更新 Job 对象的状态为 Finished。</p><p>（这有点像温度自动调节器关闭了一个灯，以此来告诉你房间的温度现在到你设定的值了）。</p><h3 id="直接控制"><a href="#直接控制" class="headerlink" title="直接控制"></a>直接控制</h3><p>相比 Job 控制器，有些控制器需要对集群外的一些东西进行修改。</p><p>例如，如果你使用一个控制环来保证集群中有足够的<a href="https://kubernetes.io/docs/concepts/architecture/nodes/" target="_blank" rel="noopener">节点</a>，那么控制就需要当前集群外的一些服务在需要时创建新节点。</p><p>和外部状态交互的控制器从 API 服务器获取到它想要的状态，然后直接和外部系统进行通信并使当前状态更接近期望状态。</p><p>（实际上有一个控制器可以水平地扩展集群中的节点。请参阅 <a href="https://kubernetes.io/docs/tasks/administer-cluster/cluster-management/#cluster-autoscaling" target="_blank" rel="noopener">集群自动扩缩容</a>）。</p><h2 id="设计"><a href="#设计" class="headerlink" title="设计"></a>设计</h2><p>作为设计原则之一，Kubernetes 使用了很多控制器，每个控制器管理集群状态的一个特定方面。 最常见的一个特定的控制器使用一种类型的资源作为它的期望状态， 控制器管理控制另外一种类型的资源向它的期望状态演化。</p><p>使用简单的控制器而不是一组相互连接的单体控制回路是很有用的。 控制器会失败，所以 Kubernetes 的设计正是考虑到了这一点。</p><blockquote><p>说明：<br>可以有多个控制器来创建或者更新相同类型的对象。 在后台，Kubernetes 控制器确保它们只关心与其控制资源相关联的资源。</p><p>例如，你可以创建 Deployment 和 Job；它们都可以创建 Pod。 Job 控制器不会删除 Deployment 所创建的 Pod，因为有信息 （<a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/" target="_blank" rel="noopener">标签</a>）让控制器可以区分这些 Pod。</p></blockquote><h2 id="运行控制器的方式"><a href="#运行控制器的方式" class="headerlink" title="运行控制器的方式"></a>运行控制器的方式</h2><p>Kubernetes 内置一组控制器，运行在 <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/" target="_blank" rel="noopener">kube-controller-manager</a> 内。 这些内置的控制器提供了重要的核心功能。</p><p>Deployment 控制器和 Job 控制器是 Kubernetes 内置控制器的典型例子。 Kubernetes 允许你运行一个稳定的控制平面，这样即使某些内置控制器失败了， 控制平面的其他部分会接替它们的工作。</p><p>你会遇到某些控制器运行在控制面之外，用以扩展 Kubernetes。 或者，如果你愿意，你也可以自己编写新控制器。 你可以以一组 Pod 来运行你的控制器，或者运行在 Kubernetes 之外。 最合适的方案取决于控制器所要执行的功能是什么。</p><h2 id="举例说明"><a href="#举例说明" class="headerlink" title="举例说明"></a>举例说明</h2><p>以 <strong>Deployment</strong> 为例:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-deployment</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">nginx:1.7.9</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure><p>该 Deployment 定义的编排动作要求: 确保携带了 <strong>app=nginx</strong> 标签的 Pod 的个数，永远等于 <strong>spec.replicas</strong> 指定的个数，即 <strong>2</strong> 个。</p><p>集群会根据携带 <strong>app=nginx</strong> 标签的 <strong>Pod</strong> 的实际数量来执行创建或者删除 Pod 操作，使数量收敛于 <strong>2</strong>。</p><p>这时，你也许就会好奇：究竟是 Kubernetes 项目中的哪个组件，在执行这些操作呢？</p><p>在上一小节提到的 <strong>kube-controller-manager</strong> 就是负责管理 <strong>Controllers</strong> 的服务组件。并且，<strong>Kubernetes</strong> 项目已经包含了若干的 <strong>Controllers</strong>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> kubernetes/pkg/controller/</span><br><span class="line">$ ls -d */</span><br><span class="line">deployment/             job/                    podautoscaler/</span><br><span class="line">cloud/                  disruption/             namespace/</span><br><span class="line">replicaset/             serviceaccount/         volume/</span><br><span class="line">cronjob/                garbagecollector/       nodelifecycle/          replication/            statefulset/            daemon/</span><br></pre></td></tr></table></figure><h3 id="控制循环"><a href="#控制循环" class="headerlink" title="控制循环"></a>控制循环</h3><p>正如本文开篇所说，<strong>Kubernetes</strong> 的 <strong>Controllers</strong> 遵循 <strong>控制回路（Control Loop）</strong> 的工作模式。</p><p>比如，对于编排的对象 X，可以用一段 Go 语言风格的伪代码，来描述其 Controller 的控制循环：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> &#123;</span><br><span class="line">    实际状态 := 获取集群中对象X的实际状态（Actual State）</span><br><span class="line">    期望状态 := 获取集群中对象X的期望状态（Desired State）</span><br><span class="line">    <span class="keyword">if</span> 实际状态 == 期望状态&#123;</span><br><span class="line">        什么都不做</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        执行编排动作，将实际状态调整为期望状态</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>一般情况:</p><ul><li><strong>实际状态来自于 Kubernetes 集群本身</strong>: 比如，kubelet 通过心跳汇报的容器状态和节点状态，或者监控系统中保存的应用监控数据，或者控制器主动收集的它自己感兴趣的信息，这些都是常见的实际状态的来源。</li><li><strong>期望状态来自于用户提交的 YAML 文件</strong>: 比如，Deployment 对象中 Replicas 字段的值。很明显，这些信息往往都保存在 Etcd 中。</li></ul><p>具体到本示例，Deployment 控制器的工作流程为:</p><ol><li>从 Etcd 中获取到所有携带了 <strong>app: nginx</strong> 标签的 Pod，统计其数量，作为实际状态</li><li>从 Template 中获取 Deployment 对象的 <strong>spec.replicas</strong> 值，作为期望状态</li><li>将两个状态做比较，然后根据比较结果，确定是创建 Pod，还是删除已有的 Pod</li></ol><p>以上即为 <strong>Kubernetes Controller</strong> 的工作模式。</p><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="https://kubernetes.io/docs/concepts/architecture/controller/" target="_blank" rel="noopener">Architecture-Controllers</a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/" target="_blank" rel="noopener">Workloads-Controllers</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在机器人技术和自动化领域，控制回路（Control Loop）是一个非终止回路，用于调节系统状态。&lt;/p&gt;
&lt;p&gt;这是一个控制环的例子：房间里的温度自动调节器。&lt;/p&gt;
&lt;p&gt;当你设置了温度，告诉了温度自动调节器你的期望状态（Desired State）。 房间的实际温度是</summary>
      
    
    
    
    
    <category term="kubernetes" scheme="http://divinerapier.github.io/tags/kubernetes/"/>
    
    <category term="controllers" scheme="http://divinerapier.github.io/tags/controllers/"/>
    
  </entry>
  
  <entry>
    <title>使用 NFS</title>
    <link href="http://divinerapier.github.io/2020/10/02/how-to-use-nfs/"/>
    <id>http://divinerapier.github.io/2020/10/02/how-to-use-nfs/</id>
    <published>2020-10-02T08:18:20.000Z</published>
    <updated>2020-10-06T06:37:26.440Z</updated>
    
    <content type="html"><![CDATA[<p>查看被分享目录的属性:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">stat</span> /public</span><br><span class="line">  File: /public</span><br><span class="line">  Size: 4096            Blocks: 8          IO Block: 4096   directory</span><br><span class="line">Device: 802h/2050d      Inode: 58982401    Links: 2</span><br><span class="line">Access: (0777/drwxrwxrwx)  Uid: (    0/    root)   Gid: (    0/    root)</span><br><span class="line">Access: 2020-10-02 16:25:03.129246127 +0800</span><br><span class="line">Modify: 2020-10-02 16:24:56.129203697 +0800</span><br><span class="line">Change: 2020-10-02 16:24:56.129203697 +0800</span><br><span class="line"> Birth: -</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo exportfs -avrf</span><br><span class="line">exporting *:/public</span><br></pre></td></tr></table></figure><p>服务端 <strong>/etc/exports</strong> 配置如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;public *(rw,sync,no_subtree_check)</span><br></pre></td></tr></table></figure><p>确认被 NFS 导出的本地文件系统:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo exportfs -avrf</span><br><span class="line">exporting *:/public</span><br></pre></td></tr></table></figure><p>在客户端查看远端配置:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ showmount -e 192.168.50.5</span><br><span class="line">Export list <span class="keyword">for</span> 192.168.50.5:</span><br><span class="line">/public                               *</span><br></pre></td></tr></table></figure><p>客户端挂载 NFS 文件系统:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo mount -o rw,nolock -t nfs 192.168.50.5:/public ./tmp</span><br></pre></td></tr></table></figure><p>当出现如下报错信息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mount.nfs: Operation not permitted</span><br></pre></td></tr></table></figure><p>请修改 <strong>/etc/exports</strong> 内容为:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;public *(rw,sync,all_squash,no_subtree_check,insecure)</span><br></pre></td></tr></table></figure><p>然后重新执行命令挂载。</p><p>如果希望指定 <code>user</code> 与 <code>group</code> 来操作文件，可以通过在 <strong>/etc/exports</strong> 中增加选项: <strong>anonuid=1026,anongid=100</strong>。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://unix.stackexchange.com/questions/252812/user-permissions-in-nfs-mounted-directory" target="_blank" rel="noopener">User permissions in NFS mounted directory</a></li><li><a href="https://access.redhat.com/solutions/3773891" target="_blank" rel="noopener">Mount failed with mount: mount.nfs: access denied by server while mounting error</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;查看被分享目录的属性:&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/sp</summary>
      
    
    
    
    
    <category term="nfs" scheme="http://divinerapier.github.io/tags/nfs/"/>
    
    <category term="filesystem" scheme="http://divinerapier.github.io/tags/filesystem/"/>
    
  </entry>
  
</feed>
